---
title: "Tom's Statistics and Data Science Toolkit (tsdst)"
subtitle: "Version 1.0.11"
author: "Tom Werner"
date: "Last Updated: `r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: true
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 5
bibliography: "bibliography.bib"
---

```{r, r_options, include=FALSE}
knitr::opts_chunk$set(comment = NA)
library("reticulate")
envs <- conda_list()[[1]]
cur_env <- envs[which(envs == "tsdst-dev")]
use_condaenv(cur_env)
print(cur_env)
py_run_string("import os as os")
py_run_string("os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/tomwe/Anaconda3/Library/plugins/platforms'")

```

```{python, import_tsdst, echo=FALSE, eval=TRUE, include=FALSE}
#engine.path="C:\\Users\\j76747\\Anaconda3\\envs\\tsdst\\python.exe"
import numpy as np
import pandas as pd
import sys
sys.path.append("C:/Users/tomwe/PycharmProjects/tsdst/")
import tsdst
```


# Introduction

This document covers all the functions, classes, and methods contained in the `tsdst` package. It is organized in alphabetical order, first by module name, and then by object name. Many of the methods discussed have general uses, however, most of the examples will be directly related to the reliability of mechanical systems. This document assumes a basic knowledge Bayesian Probability Theory. For more info, see the following [Wikipedia article.](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem")

# The Package

## tsdst.distributions

This section contains statistical functions for various distributions for a wide range of purposes.

### General Functions

#### dpoibin_exact

The probability mass function for the poisson-binomial distribution.

This function is adapted from Mark Lodato of StackExchange, Nov 7'16. It's fast, but runs slower than 1 second once `len(p) > 25000`. Computes both the pdf and the cdf. See [@poibin] or [@poibin_exact] for more information.

Adapted to compute both the pdf and the cdf.

    Parameters
    ----------
    k : numpy array or int
        Poisson counts.
    p : numpy array or float
        Binomial probabilities.
    cdf : bool, optional
        Compute the cdf. Default is False.

    Returns
    -------
    karray : float or numpy array
        The mass of the poisson-binomial distribution.

```{python, dpoibin_exact, eval=TRUE}
import numpy as np
from tsdst.distributions import dpoibin_exact

p = np.array([0.5,0.975, 0.3])
k = np.array([2, 3])

print(dpoibin_exact(k, p, cdf=False))
print(dpoibin_exact(k, p, cdf=True))
```

#### dpoibin_FT

The probability mass function for the poisson-binomial distribution. This function is written by the author of this package, and uses the discrete fourier transform.

    Parameters
    ----------
    k : numpy array or int
        Poisson counts.
    p : numpy array or float
        Binomial probabilities.

    Returns
    -------
    karray : float or numpy array
        The mass of the poisson-binomial distribution.

```{python, dpoibin_FT, eval=TRUE}
import numpy as np
from tsdst.distributions import dpoibin_FT

p = np.array([0.5,0.975, 0.3])
k = np.array([2, 3])

dpoibin_FT(k, p)
```

#### dpoibin_PA

Poisson approximation of the Poisson-Binomial probability mass function. Fast, but possibly inaccurate.

    Parameters
    ----------
    k : numpy array or int
        The poisson counts.
    p : numpy array or float
        Binomial probabilities.

    Returns
    -------
    prob : float
        The mass of the poisson-binomial distribution.

```{python, dpoibin_PA, eval=TRUE}
import numpy as np
from tsdst.distributions import dpoibin_PA

p = np.array([0.5,0.975, 0.3])
k = np.array([2, 3])

dpoibin_PA(k, p)
```

#### dwrap

dwrap is intended to function much like the R distribution functions, and is mostly a wrapper for different SciPy functions. `data` represents the random variable going into the distribution. `params` is an array of parameter values for the distribution, (generally in the order of (shape, scale) or (mu, sigma)). `disttype` lets you pick between `pdf` for the probability density function, `cdf` for the cumulative density function, or `sf` for the survival function. Weibull is the exception and also includes `inv-cdf` for the inverse cdf, `left-truncated-cdf`, and 'left-truncated-inv-cdf'. Distributions included are:

* weibull
* exponential
* log-normal (lnorm)
* normal (norm)
* gamma

I have found that writing the distributions in plain math is sometimes faster in python than using the scipy implementations, which is why this function originally existed. I wrote my own versions of the distributions, except where the distributions were complicated and it wasn't worth it at the time (and then this function uses scipy). However, as time goes on, that will probably change. Also, the scipy.stats implementations and documentations are quite complete, so unless you're feeling adventurous, it's probably a good idea to just use [scipy](https://docs.scipy.org/).


    Parameters
    ----------
    data : numpy array or pandas dataframe/series
        Numeric values representing the data of interest, either a random
        variable, probability, or quantile.
    params : numpy array or pandas dataframe/series
        the parameters of the function of interest (shape, scale, etc.)
        - weibull: (shape, scale)
        - exponential: (rate,)
        - lnorm: (mu, sigma)
        - normal: (mu, sigma)
        - gamma: (shape, scale)
    disttype : str
        the distribution type, which currently includes pdf, cdf, inv-cdf, sf,
        left-truncated-cdf, left-truncated-inv-cdf. (Note: not all of these
        options may be available for all funct options)
    funct : str
        the distribution function, which currently includes weibull, 
        exponential, log-normal (as lnorm), normal, and gamma
    log : bool, optional
        Whether to use the log of the distribution or not. The default is
        False.
    
    Raises
    ------
    ValueError
        Raised when invalid distribution type or function is chosen.
    
    Returns
    -------
    numpy array 
        an array containing the evaluation of the distribution.

```{python, dwrap, eval=TRUE}
import numpy as np
from tsdst.distributions import dwrap

data = np.array([0.5,0.975])
params = np.array([0, 1])
dwrap(data, params, disttype="inv-cdf", funct="normal", log=False)
```

#### pnorm_approx

This is a fast approximation of the normal cdf, accurate to 1.5e-7. For exact solutions, use [dwrap] or SciPy.

    Parameters
    ----------
    q : float
        Quantile of interest.
    mu : float, optional
        Mean of the normal distribution. The default is 0.
    sigma : float, optional
        Standard deviation of the normal distribution. The default is 1.
    lt : bool, optional
        Use lower tail. The default is True.
    log : bool, optional
        Return log of the value. The default is False.

    Returns
    -------
    prob : float
        The probability at the given quantile.

```{python, pnorm_approx, eval=TRUE}
import numpy as np
from tsdst.distributions import pnorm_approx

data = np.array([0, 1.96])
params = np.array([0, 1])

pnorm_approx(q=data, mu=0, sigma=1, lt=True, log=False)
```

#### ppoibin_RNA

Refined normal approximation of Poisson-Binomial Cumulative Distribution. This code is adapted from the R package 'poibin'. Very fast for len(p) > 10000.

    Parameters
    ----------
    k : numpy array or int
        Poisson counts.
    p : numpy array or float
        Binomial probabilities.
    p_weight : numpy array, optional
        Weights for the probabilities. Same length as p. The default is None.

    Returns
    -------
    vkk_r : numpy array or float
        cumulative probabilities.

```{python, ppoibin_RNA, eval=TRUE}
import numpy as np
from tsdst.distributions import ppoibin_RNA

p = np.array([0.5,0.975, 0.3])
k = np.array([2, 3])

ppoibin_RNA(k, p, p_weight=None)
```

#### qnorm_aprox

This is a fast approximation of the normal quantile function, or inverse cdf. Accurate to 2 or 3 digits. For exact solutions, use [dwrap] or SciPy.

    Parameters
    ----------
    p : float or array-like
        percentile (or probability) of interest.
    mu : float, optional
        Mean of the normal distribution. The default is 0.
    sigma : float, optional
        Standard deviation of the normal distribution. The default is 1.
    lt : bool, optional
        Lower tail of the distribution. The default is True.

    Returns
    -------
    quant : float or numpy array
        Return quantile of interest.

```{python, qnorm_approx, eval=TRUE}
import numpy as np
from tsdst.distributions import qnorm_aprox

data = np.array([0.5,0.975])

qnorm_aprox(p=data, mu=0, sigma=1, lt=True)
```

### Likelihood Functions

#### likelihood_bernoulli

The likelihood for a binary output or bernoulli model 
    
    Parameters
    ----------
    y_true : numpy array (numeric)
        The true values (the binary observations).
    y_score : numpy array or pandas dataframe
        The predicted probabilities.
    neg : bool, optional
        Return negative likelihood. The default is True.
    log : bool, optional
        Return log-likelihood. The default is True.

    Returns
    -------
    float
        The likelihood.

#### likelihood_gaussian

The likelihood for a normal(ish) output or gaussian model 
    
    Parameters
    ----------
    y_true : numpy array (numeric)
        The true values (the normal observations).
    y_score : numpy array or pandas dataframe
        The estimated value/values (mean of the normal distribution).
    neg : bool, optional
        Return negative likelihood. The default is True.
    log : bool, optional
        Return log-likelihood. The default is True.

    Returns
    -------
    float
        The likelihood.

#### likelihood_poisson

The likelihood for a count output or poisson model 
    
    Parameters
    ----------
    y_true : numpy array (numeric)
        The true values (the poisson observations).
    y_score : numpy array or pandas dataframe
        The estimated value/values (lambda).
    neg : bool, optional
        Return negative likelihood. The default is True.
    log : bool, optional
        Return log-likelihood. The default is True.

    Returns
    -------
    float
        The likelihood.

#### glm_likelihood_bernoulli

The likelihood for a logistic regression or bernoulli model with a penalty term (can accept any norm, default is 1 for L1).

    Parameters
    ----------
    parms : numpy array (numeric)
        The coefficients (including intercept, which is first)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    lamb : int, optional
        The size of the penalty (lambda). Note this is the inverse of the
        common sklearn parameter C (i.e. C=1/lambda. The default is 1.
    l_p : int, optional
        The mathmatical norm to be applied to the coefficients.
        The default is 1, representing an L1 penalty.
    neg : bool, optional
        Return negative likelihood. The default is True.
    log : bool, optional
        Return log-likelihood. The default is True.

    Returns
    -------
    float
        The negative log-likelihood.

```{python, glm_likelihood_bernoulli, eval=TRUE}
import numpy as np
from tsdst.distributions import glm_likelihood_bernoulli

intercept = np.array([3])
betas = np.array([2,4,5])

params = np.concatenate((intercept, betas))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=1, size=100))

glm_likelihood_bernoulli(params, X, Y, lamb=1, l_p=1)
```

#### glm_likelihood_gaussian

The likelihood for a gaussian regression model with a penalty term (can accept any norm, default is 1 for L1)

    Parameters
    ----------
    parms : numpy array (numeric)
        The coefficients (including intercept, which is first)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    lamb : int, optional
        The size of the penalty (lambda). Note this is the inverse of the
        common sklearn parameter C (i.e. C=1/lambda. The default is 1.
    l_p : int, optional
        The mathmatical norm to be applied to the coefficients.
        The default is 1, representing an L1 penalty.
    neg : bool, optional
        Return negative likelihood. The default is True.
    log : bool, optional
        Return log-likelihood. The default is True.

    Returns
    -------
    float
        The negative log-likelihood.
        
```{python, glm_likelihood_gaussian, eval=TRUE}
import numpy as np
from tsdst.distributions import glm_likelihood_gaussian

intercept = np.array([3])
betas = np.array([2,4,5])

params = np.concatenate((intercept, betas))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.random.normal(size=100)

glm_likelihood_gaussian(params, X, Y, lamb=1, l_p=1)
```

#### glm_likelihood_poisson

The likelihood for a poisson regression model with a penalty term (can accept any norm, default is 1 for L1)

    Parameters
    ----------
    parms : numpy array (numeric)
        The coefficients (including intercept, which is first)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    lamb : int, optional
        The size of the penalty (lambda). Note this is the inverse of the
        common sklearn parameter C (i.e. C=1/lambda. The default is 1.
    l_p : int, optional
        The mathmatical norm to be applied to the coefficients.
        The default is 1, representing an L1 penalty.
    neg : bool, optional
        Return negative likelihood. The default is True.
    log : bool, optional
        Return log-likelihood. The default is True.

    Returns
    -------
    float
        The negative log-likelihood.

```{python, glm_likelihood_poisson, eval=TRUE}
import numpy as np
from tsdst.distributions import glm_likelihood_poisson

intercept = np.array([3])
betas = np.array([2,4,5])

params = np.concatenate((intercept, betas))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=10, size=100))

glm_likelihood_poisson(params, X, Y, lamb=1, l_p=1)
```

#### ExactMLE_exp

Calculates the exact solution to the maximum likelihood of the exponential distribution (see [Appendix] for more details). This function also allows for type I right-censored data. The `censored` variable is defined as whether the data was successfully observed or not. For example, if a failure occurred for a certain data point, then `censored=1`, otherwise if no failure was observed, `censored=0`. Therefore, `censored` is an array of 1s and 0s.

    Parameters
    ----------
    data : numpy array
        The data for the exponential distribution (usually time between events,
        or similar).
    censored : numpy array
        An array of boolean values where 1 means the event occured and 0 means
        the event is censored (right).

    Returns
    -------
    list
        A list containing (in order) the MLE, the standard error, and the 
        95% confidence interval for the estimate.

```{python eval=FALSE}
ExactMLE_exp(data, censored)
```

### Posterior Functions

#### posterior_logreg_lasso

The posterior density for a logistic regression model with an L1 penalty term.

    Parameters
    ----------
    parms : numpy array (numeric)
        The coefficients (including intercept, which is first)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    l_scale : float, optional
        The value of the scale parameter in the Laplace distribution.
        A common choice for the laplace prior is scale = 2/lambda, 
        because it can be shown that this is the MAP estimate where
        a laplace prior is equivalent to performing lasso regression.
        lambda is the L1 penalty, or scale = 2*C (where C is the penalty term
        in sklearn).
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this. This parameterization is similar to
        scale = stddev/lambda or scale = stddev*C, where they set stddev to
        1 instead of 2. The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)
        

```{python, posterior_logreg_lasso, eval=TRUE}
import numpy as np
from tsdst.distributions import posterior_logreg_lasso

intercept = np.array([3])
betas = np.array([2,4,5])

params = np.concatenate((intercept, betas))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=1, size=100))

posterior_logreg_lasso(params, X, Y)
```

#### ap_logreg_lasso

The adaptive posterior density for a logistic regression model with an L1 penalty term. However, unlike the posterior_logreg_lasso function, this is made to addaptively learn the optimal L1 penalty. Therefore, the L1 penalty is in the parms variable, at the end of the array


    Parameters
    ----------
    parms : numpy array (numeric)
        The model coefficients (including intercept, which is first, and the
        scale of the laplace distribution, which is last)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    l_scale : float, optional
        ---THIS IS NOT USED. ONLY HERE FOR CONVENIENCE OF THE USER WHEN
        EXPERIMENTING BETWEEN THE ADAPTIVE AND NON-ADAPTIVE VERSIONS--- 
        The value of the scale parameter in the Laplace distribution.
        A common choice for the laplace prior is scale = 2/lambda, 
        because it can be shown that this is the MAP estimate where
        a laplace prior is equivalent to performing lasso regression.
        lambda is the L1 penalty, or scale = 2*C (where C is the penalty term
        in sklearn).
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this. This parameterization is similar to
        scale = stddev/lambda or scale = stddev*C, where they set stddev to
        1 instead of 2. The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)
    
```{python, ap_logreg_lasso, eval=TRUE}
import numpy as np
from tsdst.distributions import ap_logreg_lasso

intercept = np.array([3])
betas = np.array([2,4,5])
scale_param = np.array([1])

params = np.concatenate((intercept, betas, scale_param))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=1, size=100))

ap_logreg_lasso(params, X, Y)
```

#### ap_poisson_lasso

The adaptive posterior density for a poisson regression model with an L1 penalty term. However, unlike the poisson_regression function, this is made to adaptively learn the optimal L1 penalty. Therefore, the L1 penalty is in the `parms` variable, at the end of the array

    Parameters
    ----------
    parms : numpy array (numeric)
        The model coefficients (including intercept, which is first, and the
        scale of the laplace distribution, which is last)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    l_scale : float, optional
        ---THIS IS NOT USED. ONLY HERE FOR CONVENIENCE OF THE USER WHEN
        EXPERIMENTING BETWEEN THE ADAPTIVE AND NON-ADAPTIVE VERSIONS--- 
        The value of the scale parameter in the Laplace distribution.
        A common choice for the laplace prior is scale = 2/lambda, 
        because it can be shown that this is the MAP estimate where
        a laplace prior is equivalent to performing lasso regression.
        lambda is the L1 penalty, or scale = 2*C (where C is the penalty term
        in sklearn).
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this. This parameterization is similar to
        scale = stddev/lambda or scale = stddev*C, where they set stddev to
        1 instead of 2. The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)
    
```{python, ap_poisson_lasso, eval=TRUE}
import numpy as np
from tsdst.distributions import ap_poisson_lasso

intercept = np.array([3])
betas = np.array([2,4,5])
scale_param = np.array([1])

params = np.concatenate((intercept, betas, scale_param))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=10, size=100))

ap_poisson_lasso(params, X, Y)
```

#### ap_poisson_lasso_od

The posterior density for a poisson regression model with an L1 penalty term. However, unlike the poisson_regression function, this is made to addaptively learn the optimal L1 penalty. Therefore, the L1 penalty is in the parms variable, at the end of the array. This function also attempts to adaptively account for overdispersion (See [Appendix] for more details).

    Parameters
    ----------
    parms : numpy array (numeric)
        The model coefficients (including intercept, which is first, and the
        scale of the laplace distribution, which is last)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    l_scale : float, optional
        ---THIS IS NOT USED. ONLY HERE FOR CONVENIENCE OF THE USER WHEN
        EXPERIMENTING BETWEEN THE ADAPTIVE AND NON-ADAPTIVE VERSIONS--- 
        The value of the scale parameter in the Laplace distribution.
        A common choice for the laplace prior is scale = 2/lambda, 
        because it can be shown that this is the MAP estimate where
        a laplace prior is equivalent to performing lasso regression.
        lambda is the L1 penalty, or scale = 2*C (where C is the penalty term
        in sklearn).
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this. This parameterization is similar to
        scale = stddev/lambda or scale = stddev*C, where they set stddev to
        1 instead of 2. The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)

```{python, ap_poisson_lasso_od, eval=TRUE}
import numpy as np
from tsdst.distributions import ap_poisson_lasso_od

intercept = np.array([3])
betas = np.array([2,4,5])
scale_param = np.array([1])
od_param = np.array([2])

params = np.concatenate((intercept, betas, scale_param, od_param))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=10, size=100))

ap_poisson_lasso_od(params, X, Y)
```

#### posterior_poisson_lasso

The posterior density for a poisson regression model with an L1 penalty term.

    Parameters
    ----------
    parms : numpy array (numeric)
        The model coefficients (including intercept, which is first, and the
        scale of the laplace distribution, which is last)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    l_scale : float, optional
        The value of the scale parameter in the Laplace distribution.
        A common choice for the laplace prior is scale = 2/lambda, 
        because it can be shown that this is the MAP estimate where
        a laplace prior is equivalent to performing lasso regression.
        lambda is the L1 penalty, or scale = 2*C (where C is the penalty term
        in sklearn).
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this. This parameterization is similar to
        scale = stddev/lambda or scale = stddev*C, where they set stddev to
        1 instead of 2. The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)

```{python, posterior_poisson_lasso, eval=TRUE}
import numpy as np
from tsdst.distributions import posterior_poisson_lasso

intercept = np.array([3])
betas = np.array([2,4,5])

params = np.concatenate((intercept, betas))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=10, size=100))

posterior_poisson_lasso(params, X, Y, l_scale=0.5)
```

#### posterior_poisson_lasso_od

The posterior density for a poisson regression model with an L1 penalty term. This function also attempts to adaptively account for over-dispersion (See [Appendix] for more details).
 

    Parameters
    ----------
    parms : numpy array (numeric)
        The model coefficients (including intercept, which is first, and the
        scale of the laplace distribution, which is last)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    l_scale : float, optional
        ---THIS IS NOT USED. ONLY HERE FOR CONVENIENCE OF THE USER WHEN
        EXPERIMENTING BETWEEN THE ADAPTIVE AND NON-ADAPTIVE VERSIONS--- 
        The value of the scale parameter in the Laplace distribution.
        A common choice for the laplace prior is scale = 2/lambda, 
        because it can be shown that this is the MAP estimate where
        a laplace prior is equivalent to performing lasso regression.
        lambda is the L1 penalty, or scale = 2*C (where C is the penalty term
        in sklearn).
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this. This parameterization is similar to
        scale = stddev/lambda or scale = stddev*C, where they set stddev to
        1 instead of 2. The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)

```{python, posterior_poisson_lasso_od, eval=TRUE}
import numpy as np
from tsdst.distributions import posterior_poisson_lasso_od

intercept = np.array([3])
betas = np.array([2,4,5])
od_param = np.array([2])

params = np.concatenate((intercept, betas, od_param))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.uniform(low=0, high=10, size=100))

posterior_poisson_lasso_od(params, X, Y, l_scale=0.5)
```  

#### weibull_regression_post

--NOT OPERATIONAL-- The posterior density for a weibull regression model with an L1 penalty term. This function is currently not in working condition.

    Parameters
    ----------
    parms : numpy array (numeric)
        The model coefficients (including intercept, which is first, and the
        scale of the laplace distribution, which is last)
    X : numpy array (numeric)
        The independent variables (or feature matrix), where the first column
        is a dummy column of 1's (for the intercept).
    Y : numpy array or pandas dataframe
        The response value (should be 0 or 1, but could be float as well if 
        you're willing to deal with those consequences).
    status : numpy array (int, bool) or None
        Indicates status used for censoring, in this case, right censoring.
        An array of 1's and 0's, such that a 1 indicates an event, and 0 is the
        censored observation. If None, it will assume there are no censored
        events. Default is None.
    l_scale : float, optional
        The value of the scale parameter in the Laplace distribution.
        
        A common choice for the laplace prior is scale = 2/lambda, or
        scale = 2*C, where lambda is the L1 penalty (regularization strength)
        and C is the inverse penalty/strength. This is because it can be shown
        that this is the MAP estimate where a laplace prior is equivalent
        to performing lasso regression.
        
        I find that when scale == C, you get more similar results to
        the model output in sklearn, and based on
        my research into their implementation, I think it is because the
        sklearn implementation actually scales lambda by 1/2 (see the user
        guide (here)[https://scikit-learn.org/stable/modules/linear_model.html] for more details),
        but i'm not 100% sure on this, and it deserves more exploration.
        This parameterization is similar to scale = variance/lambda or
        scale = variance*C, where they set variance to 1 instead of 2.
        The default value is 1.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density)

```{python, weibull_regression_post, eval=FALSE}
# This is here for demonstration only. Currently, this will not run
import numpy as np
from tsdst.distributions import weibull_regression_post

intercept = np.array([3])
betas = np.array([2,4,5])
scale_param = np.array([1])
status = np.repeat(1, 100)

params = np.concatenate((intercept, betas, scale_param))
np.random.seed(123)
X = np.random.normal(size=(100, 3))
X = np.hstack((np.repeat(1, 100).reshape(-1, 1), X))
Y = np.round(np.random.normal(100, 10, size=100))

weibull_regression_post(params, X, Y, status, l_scale=0.5)
```

#### exp_gamma

A function to compute the posterior of a censored exponential distribution. `param` is a 1-D array containing the rate. `fails` is an array of the known failure times and `cens` is an array of the censored failure times.

This posterior assumes a gamma prior distribution on the rate.

    Parameters
    ----------
    param : float
        The rate parameter of an exponential distribution.
    data : numpy array
        The data for the exponential likelihood (Usually times).
    status : numpy array
        An array of binary variables indicating which items are censored (1's are events, 0's are unknown). If there
        are no censored observations, pass an array of 1's.
    gshape : float
        shape parameter of the gamma prior
    gscale : float
        scale parameter of the gamma prior
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    post : float
        The posterior density.

```{python, exp_gamma, eval=TRUE}
import numpy as np
from tsdst.distributions import exp_gamma

param = 3
data = np.array([1, 3, 2, 8, 4, 3, 2, 1, 2, 6, 7, 10, 9])
status = np.array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1])

exp_gamma(param, data, status, gshape=0.01, gscale=100)
```

#### NHPP_posterior

A posterior distribution of the Non-homogeneous Poisson distribution with a power-law process.


    Parameters
    ----------
    lparm : numpy array (float)
        eta and phi (shape and scale with a variable change).
    tints : numpy array (int)
        The sequence of time intervals (1,2,3, etc...) for the poisson counts.
    tbar : int
        time between tints (usually 1).
    obs : numpy array (int)
        The data (usually count) for the poisson process (equal to length as tints).
    a : float
        lower bound of the uniform prior on phi.
    b : float
        upper bound of the uniform prior on phi.
    mu : float
        This is a paramter that is part of the variable change on the gamma
        prior for the poisson likelihood.
    sigma : float
        This is a paramter that is part of the variable change on the gamma
        prior for the poisson likelihood.
    neg : bool, optional
        Return negative density. The default value is False.
    log : bool, optional
        Return log-density. The default value is True.

    Returns
    -------
    float
        The posterior density (height of the posterior density).


```{python, NHPP_posterior, eval=TRUE}
import numpy as np
from tsdst.distributions import NHPP_posterior

lparm = np.array([3.5, 2.4])
tints = np.arange(1, 95, 1)
tbar = 1
a = 0.3
b = 5.0
mu = 3.0
sigma = 100.0
obs = np.array([10.,  9., 20., 12., 11., 16.,  8., 10., 11., 15.,  5.,  8., 15.,
                16.,  7., 17., 15., 12., 15.,  7.,  8., 16., 11., 15., 12., 14.,
                12., 23., 14., 12., 18., 10.,  8., 17.,  8.,  8., 8.,  9., 11., 15.,
                12.,  6.,  7., 12.,  8., 10., 11.,  5., 13., 13.,  7., 11., 13.,  8.,
                10., 14., 11., 15.,  9.,  9., 10.,  8.,  9., 16., 11.,  5.,  8., 12., 
                7.,  9.,  7., 15., 5.,  8.,  7., 18.,  8., 16., 16., 12.,  9., 11.,
                14.,  7., 10.,  6.,  7.,  8.,  8., 11., 15., 10.,  8.])
NHPP_posterior(lparm, tints, tbar, obs, a, b, mu, sigma, neg=False,
               log=True)
```

#### pois_uniform

A function to compute the posterior of a Poisson distribution. `param` is a 1-D array containing the rate. `count` is an array of the counts for each period.

This posterior assumes a non-informative uniform prior distribution on the rate.

    Parameters
    ----------
    param : float
        log of the Poisson mean (lambda).
    count : int, or array of ints
        count of events (or poisson counts)

    Returns
    -------
    float
        The posterior density (height of the posterior density).

```{python, pois_uniform, eval=TRUE}
import numpy as np
from tsdst.distributions import pois_uniform

param = 3
data = np.array([1, 3, 2, 8, 4, 3, 2, 1, 2, 6, 7, 10, 9])

pois_uniform(param, data)
```

#### pois_gamma

Posterior density for the Poisson distribution (assuming gamma prior)

    Parameters
    ----------
    param : float
        Log of the Poisson mean (lambda).
    count : int, or array of ints
        count of events (or poisson counts)
    gshape : float
        shape parameter of the gamma prior
    gscale : float
        scale parameter of the gamma prior

    Returns
    -------
    float
        The posterior density (height of the posterior density).
        
```{python, pois_gamma, eval=TRUE}
import numpy as np
from tsdst.distributions import pois_gamma

param = 3
data = np.array([1, 3, 2, 8, 4, 3, 2, 1, 2, 6, 7, 10, 9])

pois_gamma(param, data, gshape=0.01, gscale=100)
```

#### pois_gamma_ada

Posterior density for the Poisson distribution (assuming gamma prior), learns the gamma shape/scale as part of mcmc

    Parameters
    ----------
    param : float
        Log of the Poisson mean (lambda).
    count : int, or array of ints
        count of events (or poisson counts)

    Returns
    -------
    float
        The posterior density (height of the posterior density).
        
```{python, pois_gamma_ada, eval=TRUE}
import numpy as np
from tsdst.distributions import pois_gamma_ada

param = np.array([3, 0.01, 100])
data = np.array([1, 3, 2, 8, 4, 3, 2, 1, 2, 6, 7, 10, 9])

pois_gamma_ada(param, data)
```

#### weibull_gamma

A function to compute the posterior of a censored weibull distribution. `param` is an array containing shape and scale, respectively. `time` represents a random variable, which in most reliability cases will be a time to failure. `status` is an array of 1s and 0s where 1 is a success and 0 is censored. `neg` is a toggle to return either the negative posterior or not.

This posterior assumes an uninformative gamma prior distribution on the scale and an uninformative uniform prior on the shape.

```{python, weibull_gamma, eval=TRUE}
import numpy as np
from tsdst.distributions import weibull_gamma

param = np.array([3, 2])
data = np.array([1, 3, 2, 8, 4, 3, 2, 1, 2, 6, 7, 10, 9])
status = np.array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1])

weibull_gamma(param, data, status, gshape=0.01, gscale=100)
```

## tsdst.estimators

Custom models that are compatible with sklearn, either for maximum likelihood estimation or bayesian estimation with MCMC.

### BayesLogRegClassifier

A Logistic Regression Classifier that uses MCMC to evaluate the parameters. This objects inherits from sklearn's BaseEstimator and LinearClassifierMixin.

    Parameters
    ----------
    C : float, optional
        The value for the L1 penalty. If None, the MCMC process looks for
        the optimal penalty. The default is None.
    start : numpy array (float), optional
        The starting values for the MCMC. If None, the MLE estimate is used
        (solved with sklearn). The default is None.
    niter : int, optional
        The number of MCMC samples to draw. The default is 10000.
    algo : str, optional
        The MCMC (Metropolis) algorithm to use. The default is 'rosenthal', which is 
        a method that tunes the covariance matrix after each iteration.
        Other options include 'rwm', which is a simple random metropolis
        walk with a fixed covariance matrix, and 'lap' which is another 
        adaptive method that tunes the covariance matrix every K
        iterations.
    algo_options : dict, optional
        The options to be passed to the MCMC algorithm. Include as a
        dictionary. The default is None.
    retry_sd : float, optional
        The MCMC alorithms use a Cholesky decomposition on the covariance
        matrix. In case the decomposition fails, the algorithms will 
        attempt to jitter the covaraince matrix to help it be positive 
        definite. This value determines the strength of the jittering and
        is drawn directly from a normal distribution with zero mean and
        retry_sd standard deviation. The default is 0.02.
    retry_max_tries : int, optional
        Number of attempts to correct the cholesky decomposition if it
        fails. The default is 100.
    initialize_weights : str, optional
        Determines the method of initializing the starting model parameter
        values (if start is None). Options are 'sklearn', 'ones', 'random',
        or 'zeros'. The default is 'sklearn'.
    param_summary : str, optional
        The method used in making the final parameter summaries. Options 
        are 'mean', 'median', 'mode_kde', 'mode_histogram', or
        'final_sample'. The default is 'mean'.
    has_constant : bool, optional
        Whether or not the data provided already has a column of ones
        as the first column in the dataset for the intercept of the model.
        If not, one is created. The default is False.
    verbose : bool, optional
        If True, a progress bar, along with timestamps, is provided.
        The default is True.
    over_dispersion : bool, optional
        ---CURRENTLY NOT IMPLEMENTED---
        Whether or not to account for overdispersion in the model.
        The default is False.
    scorer : function
        The function to use for the default scoring method. Otherwise,
        pass None for accuracy. The default is None.

    Returns
    -------
    None.
    
```{python, BayesLogRegClassifier, eval=TRUE}
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from timeit import default_timer as dt

from tsdst.estimators import BayesLogRegClassifier
from tsdst.utils import print_time

X, y = make_classification(n_samples=1000, n_features=8, flip_y=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

bayes_mod = BayesLogRegClassifier(C=0.5, start=None, niter=100000, algo='rosenthal',
                      initialize_weights='random', param_summary='mean',
                      algo_options={'progress': False})
sk_mod = LogisticRegression(C=0.5)


bayes_fit = bayes_mod.fit(X_train, y_train)

t0 = dt()
sk_fit = sk_mod.fit(X_train, y_train)
t1 = dt()

print_time("Finished coordinate descent (MLE approach)...", te=t1, ts=t0)

bayes_fit.coef_
sk_fit.coef_

bayes_fit.score(X_test, y_test)
sk_fit.score(X_test, y_test)

plt.figure()
plt.plot(bayes_mod.mcmc_params[:, 1])
plt.xlabel("MCMC Sample Number")
plt.ylabel("Coefficient Value")
plt.title("Trace Plot for 1st Coefficient")
plt.show()

plt.figure()
sns.kdeplot(bayes_mod.mcmc_params[:, 1], shade=True)
plt.ylabel("Density")
plt.xlabel("Coefficient Value Across Samples")
plt.title("Density Plot for 1st Coefficient")
plt.show()
```

### BayesPoissonRegressor

A Poisson Regressor that uses MCMC to evaluate the parameters. This objects inherits from sklearn's BaseEstimator and LinearClassifierMixin.

    Parameters
    ----------
    C : float, optional
        The value for the L1 penalty. If None, the MCMC process looks for
        the optimal penalty. The default is None.
    start : numpy array (float), optional
        The starting values for the MCMC. If None, the MLE estimate is used
        (solved with sklearn). The default is None.
    niter : int, optional
        The number of MCMC samples to draw. The default is 10000.
    algo : str, optional
        The MCMC (Metropolis) algorithm to use. The default is 'rosenthal', which is 
        a method that tunes the covariance matrix after each iteration.
        Other options include 'rwm', which is a simple random metropolis
        walk with a fixed covariance matrix, and 'lap' which is another 
        adaptive method that tunes the covariance matrix every K
        iterations.
    algo_options : dict, optional
        The options to be passed to the MCMC algorithm. Include as a
        dictionary. The default is None.
    retry_sd : float, optional
        The MCMC alorithms use a Cholesky decomposition on the covariance
        matrix. In case the decomposition fails, the algorithms will 
        attempt to jitter the covaraince matrix to help it be positive 
        definite. This value determines the strength of the jittering and
        is drawn directly from a normal distribution with zero mean and
        retry_sd standard deviation. The default is 0.02.
    retry_max_tries : int, optional
        Number of attempts to correct the cholesky decomposition if it
        fails. The default is 100.
    initialize_weights : str, optional
        Determines the method of initializing the starting model parameter
        values (if start is None). Options are 'sklearn', 'ones', 'random',
        or 'zeros'. The default is 'sklearn'.
    param_summary : str, optional
        The method used in making the final parameter summaries. Options 
        are 'mean', 'median', 'mode_kde', 'mode_histogram', or
        'final_sample'. The default is 'mean'.
    has_constant : bool, optional
        Whether or not the data provided already has a column of ones
        as the first column in the dataset for the intercept of the model.
        If not, one is created. The default is False.
    verbose : bool, optional
        If True, a progress bar, along with timestamps, is provided.
        The default is True.
    over_dispersion : bool, optional
        ---CURRENTLY NOT IMPLEMENTED---
        Whether or not to account for overdispersion in the model.
        The default is False.

    Returns
    -------
    None.

```{python, BayesPoissonRegressor, eval=TRUE}
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.datasets import make_regression
from sklearn.linear_model import PoissonRegressor
from sklearn.model_selection import train_test_split
from timeit import default_timer as dt

from tsdst.estimators import BayesPoissonRegressor
from tsdst.utils import print_time

X, y = make_regression(n_samples=1000, n_features=8, bias=5, noise=0.05, random_state=42)
y = np.round((y-y.min())/(y.max()-y.min())*(50)).astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

bayes_mod = BayesPoissonRegressor(C=9999999, start=None, niter=50000, algo='rosenthal',
                      initialize_weights='sklearn', param_summary='median',
                      algo_options={'progress': False}, over_dispersion=False)
sk_mod = PoissonRegressor(alpha=0)


bayes_fit = bayes_mod.fit(X_train, y_train)

t0 = dt()
sk_fit = sk_mod.fit(X_train, y_train)
t1 = dt()

print_time("Finished coordinate descent (MLE approach)...", te=t1, ts=t0)

bayes_fit.coef_
sk_fit.coef_

bayes_fit.score(X_test, y_test)
sk_fit.score(X_test, y_test)

plt.figure()
plt.plot(bayes_mod.mcmc_params[:, 1])
plt.xlabel("MCMC Sample Number")
plt.ylabel("Coefficient Value")
plt.title("Trace Plot for 1st Coefficient")
plt.show()

plt.figure()
sns.kdeplot(bayes_mod.mcmc_params[:, 1], shade=True)
plt.ylabel("Density")
plt.xlabel("Coefficient Value")
plt.title("Density Plot for 1st Coefficient")
plt.show()
```

### BayesWeibullRegressor

Not yet implemented (coming soon)

## tsdst.feature_selection

This module contains feature selection methods for machine learning or statistical models.

### naiveVarDrop

Drop columns based on which columns have variance below the threshold.

    Parameters
    ----------
    X : pandas dataframe
        Feature (or design) matrix.
    searchCols : list or list-like (str)
        The columns to search. If None, use all columns. Default is None.
    tol : float, optional
        The threshold for variance to decide which columns to keep or drop.
        The default is 0.0001.
    asList : bool, optional
        Return only the list of columns to be dropped. The default is False.
    print_ : bool, optional
        Print the columns to be dropped. The default is False.

    Returns
    -------
    list or dataframe
        Either list of columns to be dropped or dataframe with columns removed.

### naiveScoreDrop

Drop columns based on which columns have scores below the threshold. This could be used with any arbitrary score function, where scores is a 1-column dataframe, in which the index are column names, and the values are the scores.

    Parameters
    ----------
    X : pandas dataframe
        Feature (or design) matrix.
    scores : pandas dataframe
        The score results for each column.
    tol : float, optional
        The threshold for variance to decide which columns to keep or drop.
        The default is 0.0001.
    asList : bool, optional
        Return only the list of columns to be dropped. The default is False.

    Returns
    -------
    list or dataframe
        Either list of columns to be dropped or dataframe with columns removed.
    
### getHighCorrs

Given a correlation matrix, return the correlations that are above the threshold.

To be used before [dropHighCorrs].

    Parameters
    ----------
    corr_mat : pandas dataframe
        The correlation matrix.
    corr_thres : float
        The threshold for correlation to decide which columns to keep or drop.
        Between 0 and 1.
    split_key : str, optional
        The unique key to use to join the column names together, for example,
        if split_key was '..&..', then the index for that correlation would be
        'col1..&..col2'. The default is "..&..".

    Returns
    -------
    top_corr : pandas dataframe
        A dataframe containing a list of the top correlations from the
        correlation matrix.

### dropHighCorrs

Remove high inter-correlations from the dataset. When dropping a column, the column with the lowest variance is selected.
    
To be used after [getHighCorrs].

    Parameters
    ----------
    X : pandas dataframe
        The data in tabular form (the feature or design matrix of which the 
        correlation is being evaluated).
    top_corr : dataframe
        The output from the getHighCorrs function, or, a dataframe containing a
        list of the high correlations, where the index is a list of the column
        names concatenated together by split_key, for example, 'col1..&..col2'.
    split_key : str, optional
        The unique key to use to join the column names together, for example,
        if split_key was '..&..', then the index for that correlation would be
        'col1..&..col2'. The default is "..&..".
    asList : bool, optional
        Return only the list of columns to be dropped. The default is False.
    print_ : bool, optional
        Print the columns to be dropped. The default is False.

    Returns
    -------
    list or dataframe
        Either list of columns to be dropped or dataframe with columns removed.

### dropCorrProcedure

Drops high correlation columns from a matrix. If two columns are highly correlated, it will pick one to keep based on which has higher variance.

Calculates the inter-correlation of the columns in a dataframe, and then drops columns that are above a given threshold.

    Parameters
    ----------
    X : pandas dataframe
        The data in tabular form (the feature or design matrix).
    corr_thres : float
        The threshold for correlation to decide which columns to keep or drop.
        Between 0 and 1.
    split_key : str, optional
        The unique key to use to join the column names together, for example,
        if split_key was '..&..', then the index for that correlation would be
        'col1..&..col2'. The default is "..&..".
    asList : bool, optional
        Return only the list of columns to be dropped. The default is False.
    print_ : bool, optional
        Print the columns to be dropped. The default is False.

    Returns
    -------
    dropped_cor : dataframe
        Dataframe with High correlation columns dropped.

### permutation_importance

Performs permutation importance on a fitted model.
    
Assumes that your metric function takes inputs in this order: ytrue, ypred. If it doesn't, write a simple wrapper that will. Also assumes a pretrained model with a predict method.
    
Credit: [Explained AI](https://explained.ai/rf-importance/)
    
This function was adapted from the above link.

    Parameters
    ----------
    fit_model : sklearn, (or similar)
        Can be any model that has a 'predict' method.
    Xtest : pandas Dataframe
        The observations in the feature or design matrix to perform the
        permutation test on.
    Ytest : pandas Series
        The observations in the response variable to perform the
        permutation test on.
    metric_func : function
        The function to evaluate your metric of interest.
    seed : int, optional
        Set Random Seed for reporducibility. The default is None.
    sort : str, optional
        Either 'asc' for ascending or 'dsc' for descending.
        The default is "dsc".

    Returns
    -------
    res : dataframe
        A dataframe containing the results of the permutation test.

### permutation_importance_CV

Performs a cross-validated permutation importance on a non-fitted model.
    
Assumes that your metric function takes inputs in this order: ytrue, ypred. If it doesn't, write a simple wrapper that will. Also assumes a pretrained model with a predict method.
    
Credit: [Explained AI](https://explained.ai/rf-importance/)
    
This function was adapted from the above link.

    Parameters
    ----------
    model : sklearn, (or similar)
        Can be any model that has a 'predict' method. Send to the function 
        unfitted, only instantiated
    X : pandas Dataframe
        The observations in the feature or design matrix to perform the
        permutation test on.
    Y : pandas Series
        The observations in the response variable to perform the
        permutation test on.
    metric_func : function
        The function to evaluate your metric of interest.
    num_splits : int
        Number of CV folds to use (using stratified k-fold)
    seed : int, optional
        Set Random Seed for reporducibility. The default is None.
    sort : str, optional
        Either 'asc' for ascending or 'dsc' for descending.
        The default is "dsc".

    Returns
    -------
    res : dataframe
        A dataframe containing the results of the permutation test.

### forwardSelection

A forward selection algorithm for classification only right now. Still needs some work.

    Parameters
    ----------
    XY : pandas dataframe
        The combined independent variables/features and reponse/target
        variable.
    target_var : str
        The column containing the target (or response) variable.
    model : sklearn, or similar
        An unfitted model. Any model that has a fit and predict method.
    metric : str
        The AIC metric to be used, for example, aic, aicc, bic, ebic, hastie, 
        or kwano.
    verbose : bool, optional
        Output the steps and progress as it completes. The default is True.
    n_jobs : int, optional
        If greater than 1, perform operation in parallel. The default is 1.
    early_stop : bool, optional
        Stop operations early if max number of desired selection is reached.
        The default is False.
    perc_min : TYPE, optional
        DESCRIPTION. The default is 0.05.
    stop_at_p : int, optional
        The number of selections to stop at, if early_stop is True.
        The default is 1000.
    stop_when : int, optional
        Stop the operations when the metric no longer continues to decrease,
        after evaluating the next stop_when columns. The default is 5.
    use_probabilities : bool
        Whether or not to use predicted probabilities as the only other feature
        in the set, or to use the actual features in performing the forward 
        selection.
    return_type : str, optional
        Which object to return, can be either list, model, data, or all.
        The default is 'all'.
    serialize_flavor : str
        Which mode of downsaving data to use, currently supports 'feather' and
        'msgpack'. Unfortunately, as of pandas 0.25.0, msgpack is no longer 
        supported.
        
        Note: using feather requires pyarrow

    Returns
    -------
    list, model, data, or tuple of objects
        Either return as list, model, data, or all. The default is 'all'.

```{python, forwardSelection, eval=TRUE}
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from timeit import default_timer as dt

from tsdst.feature_selection import forwardSelection


X, y = make_classification(n_samples=1000, n_features=8, flip_y=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
XY_train = pd.concat((pd.DataFrame(X_train, columns=['V' + str(i) for i in range(X.shape[1])]),
                      pd.DataFrame(y_train.reshape(-1, 1),
                                   columns=['y'])),
                      axis=1)

sk_mod = LogisticRegression(C=0.5)

forward_sel = forwardSelection(XY=XY_train, target_var='y', model=sk_mod,
                               metric='bic', verbose=True, n_jobs=1)
```

### vif_drop

Coming soon.

## tsdst.mcmc

A module for supporting MCMC evaluation capabilities, including an object to hold MCMC chains and store/compute useful metrics on them. It also includes some common plotting functionality.

The purpose of this class was to be a one-stop-shop for an MCMC chain. While algorithms or methods to produce or utilize chains may vary significantly, I have found that the methods to shape, trim, store, burn-in, or continue runs of a chain are easily generalized, and could be useful as a single class. The author is currently a proponent of the one-long-run ideology, so methods involving multiple chains are not currently considered.

When called, this class initializes the following properties:

To get a chain up and running, use the `runMCMC` method after instantiating the `mcmcObject` class.

```{python, mcmcObject}
import numpy as np
from scipy.stats import norm, invgamma
from tsdst.mcmc import mcmcObject, adaptive_mcmc


def gaussian_invGamma(params, data):
    '''
    A gaussian likelihood with a normal prior on mu, and an
    inverse-gamma prior on sigma.
    
    Parameters
    ----------
    params : numpy array
        The parameters of the distribution to be sampled in mcmc.
    data : numpy array
        The observed data.
        
    Returns
    -------
    log_post : float
        Log-Posterior density.
    '''
    mu = params[0]
    # sigma must be > 0, and params was sampled from a normal distribution,
    # so no guarantee it will be positive
    sigma = np.exp(params[1])
    
    like = np.sum(norm.logpdf(x=data, loc=mu, scale=sigma))
    
    # Weak Priors on both parameters
    norm_prior = np.sum(norm.logpdf(x=mu, loc=0, scale=100))
    invgamma_prior = np.sum(invgamma.logpdf(x=sigma, a=0.01, scale=100))
    
    # Since we are using a metropolis algorithm rather than metropolis-
    # hastings or another variant (i.e. candidate distribution is normal),
    # The Jacobian of any transformations used on the parameters must be
    # accounted for. The jacobian of an exponential transformation is
    # the log of the parameter.
    sigma_jac = params[1]
    
    log_post = like + norm_prior + invgamma_prior + sigma_jac
    return log_post


X = np.random.normal(size=(100,), loc=10, scale=3)
start = np.random.uniform(size=(2,))

normalExample = mcmcObject(name='Normal Example')
normalExample.runMCMC(start=start, initSampleSize=10000,
                      lpost=gaussian_invGamma, algo=adaptive_mcmc,
                      chainName='norm_example', plot_trace=False, plot_density=False,
                      plot_acf=False, do_raftery=True, iters_to_go=None,
                      max_iters=50000, burnin=0, lpost_args={'data': X},
                      algoOpts={'progress': False})
```

```{python, mcmcPlots, include=FALSE}
fig_d, ax_d = normalExample.plotDensity('norm_example')
fig_d.savefig("Descriptions_files/figure-html/pic_d.png")

fig_t, ax_t = normalExample.plotTrace('norm_example',
                        CTres=list(normalExample.diagnostic_results["norm_example_Convtest"]["Burn-in"]))
fig_t.savefig("Descriptions_files/figure-html/pic_t.png")

fig_a, ax_a = normalExample.plotACF('norm_example', acfType="pacf")
fig_a.savefig("Descriptions_files/figure-html/pic_a.png")

normalExample.acf('norm_example', lag=50, partial=False)
fig_pa, ax_pa = normalExample.plotACF('norm_example', acfType="acf")
fig_pa.savefig("Descriptions_files/figure-html/pic_pa.png")
```

![](Descriptions_files/figure-html/pic_d.png)

![](Descriptions_files/figure-html/pic_t.png)

![](Descriptions_files/figure-html/pic_a.png)

![](Descriptions_files/figure-html/pic_pa.png)

The `name` property is a name given to the object itself. Each chain is given it's own unique name, usually denoted as `chainName`. An object can contain multiple chains, each with its own label, so instead of having multiple related class objects, one could instantiate one object and store multiple chains within that object.

The three dictionaries have similar properties. The `chains` property can store any given MCMC run as a key-value pair, where the chain name is the key and the chain itself is the value (see [addChain] or [mcmcWithRaftery] for more info). The `diagnostic_values` and `previous_values` properties store other information about the chain, which are the diagnostic tests and the ending values of the last run, respectively.

### mcmcObject

This section contains methods relating to the `mcmcObject`.

#### __init_\_

Constructor for MCMC object

    Parameters
    ----------
    name : str, optional
        The name of the object (in case you instantiate multiple objects).
        The default is "MCMC Object".

    Returns
    -------
    None.

#### addChain

This method adds a chain to the `chains` property. If no other name is provided, the default for chainName is 'Chain_x', where 'x' is a number reflecting how many chains exist in the object. This function checks whether `newChain` is an object that can be converted to a numpy array, and if not already a numpy array, performs the conversion. Beyond that, it is up to the user to make sure what is being added has the right shape/size of your chain. If `concat` is set to true, it searches for a chain named `chainName`, and if found, appends `newChain` to the existing chain. Otherwise, a new chain is created.

    Parameters
    ----------
    newChain : numpy array
        New chain you would like to add to the collection.
    chainName : str, optional
        The name of the added chain, used to seperate it from others in the
        collection. If None, one will be selected for you.
        The default is None.
    concat : bool, optional
        Whether or not to append the new chain to an existing chain.
        The default is False.
        
    Returns
    -------
    None.

```{python eval=FALSE}
mcmcObject.addChain(newChain, chainName=None, concat=True)
```

#### bestRaftery

This function is just a wrapper for [raftery] that performs the Raftery diagnostic for each value of `q`. The `print_each` and `print_final` functions will display the output of each run of [raftery], as well as the combined result of all the runs. The combined result is simply the maximum of all the runs (see [raftery] for more details). 

    Parameters
    ----------
    chainName : str
        The name of the chain from the collection.
    q : float, optional
        Quantiles of interest (in terms of percentiles, i.e. between 0 and
                              1).
        The default is [0.025, 0.5, 0.975].
    r : float, optional
        Accuracy. The default is 0.005.
    s : float, optional
        Probability. The default is 0.95.
    converge_eps : float, optional
        Convergence threshold (epsilon). The default is 0.001.
    thin : int, optional
        Thining amount. The default is 1.
    print_each : bool, optional
        Print results at each evaluation. The default is False.
    print_final : bool, optional
        Print the final results. The default is False.

    Returns
    -------
    None.

```{python eval=FALSE}
bestRaftery(chainName, q=[0.025, 0.5, 0.975],
            r=0.005, s=0.90, converge_eps=0.001, thin=1,
            print_each=False, print_final=False)
```

#### burnin

This method removes the first `burninVal` number of samples from the chain. The default is 3000.

Some people argue that burn-in is not necessary, or even practical, since the target distribution reflects a probability distribution where those burn-in values, no matter how unlikely, are still possible to obtain as valid samples. They argue that even a parameter value that has 0.0000001% chance of being drawn is still not zero, and therefore helps make up the target distribution. From another view, you can argue that if your chain is "long enough", these values average out to be representative of the target distribution. However, the author believes that burn-in values are still useful for a similar reason. For example, it isn't always possible to run really long chains (due to hardware or software constraints, the number of functions/objects needing MCMC samples, etc.). Therefore, if someone happen to get a chain that is mixing slowly (but after looking at the plots decided it did "converge" towards the end), then the quantiles of the sampled distribution with those burn-in values included will be different, and in that case the stationary distribution may not reflect the target distribution. Furthermore, if your chain is "long enough", removing burn-in values should have little effect on the chain as a whole, and is likely to be more helpful then harmful.

    Parameters
    ----------
    chainName : str
        The name of the chain from the collection.
    burninVal : int, optional
        The number of samples to remove. The default is 3000.
    replace : bool
        If True, replace the current chain rather than create a new one
        without the burnin samples. If a new chain is created, it will
        be called chainName + '_burnin' + burnunVal, and will be available
        in the collection.

    Returns
    -------
    None.

```{python eval=FALSE}
burnin(chainName, burninVal=3000)
```

#### runMCMC

This method is intended to perform a comprehensive run and diagnostic on the MCMC chain. The method performs an initial run using [applyMCMC], which is supplied with starting values (` array start`), initial number of iterations (`int initSampleSize`), the posterior function (`func lpost`), the algorithm (`func algo`), algorithm options (`dict algoOpts`), options for [bestRaftery] (`dict raftOpts`), standard deviation for reassigning starting values during a retry (`float sd`), and the max number of retries (`int max_tries`). After the initial run completes, it adds the chain and it's ending values to the `mcmcObject`. If `chainName` is not provided or is `None`, a default name is given. See the [mcmcObject Algorithms] section for MCMC algorithms and their options that can be passed to `algoOpts`

If `do_raftery=True`, then the method will perform a second MCMC run. If `iters_to_go=None`, then the second run will be performed with the remaining number of iterations as recommended by the Raftery functions. If the recommended number of samples is already reached in the initial chain, the second run will not be performed. Otherwise, the remaining iterations will be limited by the number `max_iters` or the Raftery output, whichever is smaller. The second run is initialized using the previous values of the first run.

After all MCMC runs have completed, the function checks for burn-in. If `burnin=None`, then no burn-in occurs. Otherwise, if `burnin=0`, the burn-in is calculated using the method [TJ_Convergence_test], otherwise the value provided in `burnin` is used (note: burn-in removal from the chain occurs after plotting for display purposes).

The final step is plotting. If `plot_trace=True`, `plot_density=True`, or `plot_acf=True` then plots will be produced, and can either be written to a `jpg` file or printed to the console using the `write_plots` option. See [plotDensity], [plotTrace], or [plotACF] for more information.

The options for `raftOpts` are passed as a dictionary and are defined as follows:

```{r echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("q", "r", "s", "converge_eps", "thin", "print_each", "print_final"),
                 c("[0.025, 0.5, 0.975]", 0.005, 0.95, 0.001, 1, FALSE, FALSE),
                 c("list(floats)", "float", "float", "float" , "int", "boolean", "boolean"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

see [tsdst.mcmc] for a full example.

    Parameters
    ----------
    start : numpy array
        Starting values for the MCMC.
    initSampleSize : int
        The number of MCMC samples to draw on the first run. It's good to 
        start relatively small, because the Raftery evaluation will tell
        you how many more samples need to be drawn.
    lpost : function
        Log posterior function.
    algo : function
        The MCMC algorithm to use (could be anything, but needs to have the
    same arguments as inputs for the algorithms already defined, namely:
        start : numpy array
            Starting values for the MCMC.
        niter : int
            Number of iterations.
        lpost : function
            Log posterior function.
        postArgs : dict
            Extra arguments for the log posterior function. The default is
            an empty dictionary
        options : dict, optional
            Extra arguments for the specific MCMC algorithm
    algoOpts : dict, optional
        Extra arguments for the specific MCMC algorithm.
        The default is None.
    raftOpts : dict, optional
        A dictionary containing the options for the Raftery evaluation, 
        namely:
            q : float, optional
                Quantiles of interest (in terms of percentiles, i.e.
                                       between 0 and 1).
                The default is [0.025, 0.5, 0.975].
            r : float, optional
                Accuracy. The default is 0.005.
            s : float, optional
                Probability. The default is 0.95.
            converge_eps : float, optional
                Convergence threshold (epsilon). The default is 0.001.
            thin : int, optional
                Thining amount. The default is 1.
            print_each : bool, optional
                Print results at each evaluation. The default is False.
            print_final : bool, optional
                Print the final results. The default is False.
        The default is None.
    chainName : str, optional
        The name of the chain that will be created. If None,
        'Chain_' + int (for number of chains in the collection) 
        will be used. The default is None.
    max_tries : int, optional
        The max number of times to try and jitter before admitting defeat.
        If the jitter fails, the reason or the covaraince matrix not being
        positive definite may not be due to randomness, and may require
        a re-evaluation of the problem space. The default is 100.
    sd : float, optional
        The standard deviation of the normal distribution used to draw the
        jitter amount from. In other words, the jittered covariance is the 
        covaraince matrix plus a random draw X, where X~N(0, sd). 
        The default is 0.02.
    plot_trace : bool, optional
        Plot the trace of the MCMC samples. The default is True.
    plot_density : bool, optional
        Plot the posterior density of the MCMC samples.
        The default is True.
    plot_acf : bool, optional
        Plot the auto-correlation. The default is True.
    plot_trace_args : dict
        Arguments for the plotTrace function. Default is None.
    plot_density_args : dict
        Arguments for the plotDensity function. Default is None.
    plot_acf_args : dict
        Arguments for the plotACF function. Default is None.
    acf_type : str
        Ether 'acf', 'pacf', or None. Default is 'pacf'. If None, pacf
        calculation is not performed.
    acf_args : dict
        Arguments to pass to the `acf` function. Default is None.
    do_raftery : bool, optional
        Whether to perform the raftery evaluation, or stop after the
        first chain generation. Default is True.
    max_iters : int, optional
        The max number of new samples to draw. For example, if the Raftery
        evaluation recommends 1 million new samples, and max_iters is 
        750000, then the new samples will be restricted at 750000.
        The default is 750000.
    burnin : int, optional
        The number of initial MCMC samples to drop from the chain.
        If burnin is negative or zero, the burnin amount will be determined
        automatically. If positive, it will drop that amount. If 
        None, it will do nothing. The default is 0.
    lpost_args : dict, optional
        Any extra arguments to pass to the log posterior function.
        The default is an empty dictionary.

    Returns
    -------
    None.

```{python eval=FALSE}
runMCMC(self, start, initSampleSize, lpost, algo, algoOpts=None,
        raftOpts=None, chainName=None, max_tries=100, sd=0.02,
        plot_trace=True, plot_density=True, plot_acf=True,
        plot_trace_args=None, plot_density_args=None,
        plot_acf_args=None, acfType='pacf', acf_args=None,
        do_raftery=True, iters_to_go=None, max_iters=750000, burnin=0,
        lpost_args={})
```

#### plotTrace

`plotTrace` plots the MCMC chain of each parameter as a function of time, with each iteration on the x-axis and the sampled values on the y-axis. One benefit of looking at a trace plot is to check if the MCMC method has begun sampling around some stationary distribution. Some statisticians will say the chain "converges", but this is technically incorrect since MCMC methods are a sampling problem (jumping around a range of values) and not an optimization problem (moving to a single value around some local or global optimum), although it could be argued that the samples converge to a distribution as a whole.

    Parameters
    ----------
    chainName : str
        The name of the MCMC chain.
    CTres : numpy array, optional
        The results of the TJ_Convergence_Test. The default is None.
    write : bool, optional
        Write plot to a directory. The default is False.
    display : bool, optional
        Display the plot.
    pdir : str, optional
        The directory to write the plots to. The default is "./Plots/".
    fileType : str, optional
        The filetype of the image. The default is "png".
    figsize : tuple, optional
        The figure size (see matplotlib documentation for more details).
        The default is (15, 12).

    Returns
    -------
    fig, ax : tuple
        The figure and axes components of the plot.
        

```{python, plotTrace, eval=FALSE}
plotTrace(chainName, CTres=None, write=True, pdir="./Plots/",
          fileType="jpg", figsize=(15, 12))
```

#### plotDensity

`plotDensity` plots the posterior density of each chain parameter. This will give the user a good idea of the shape and credible bounds of each parameter, and in some cases can help diagnose convergence (for example, when the density appears jagged rather than rounded, or appears bimodal when you suspect it shouldn't be).

    Parameters
    ----------
    chainName : str
        The name of the MCMC chain.
    smoothing : float, optional
        The amount of smoothing to use on the kde plot.
        See seaborn.kde_plot for details.
        The default is 0.05.
    write : bool, optional
        Write plot to a directory. The default is False.
    display : bool, optional
        Display the plot.
    pdir : str, optional
        The directory to write the plots to. The default is "./Plots/".
    vlines : TYPE, optional
        The x-axis locations of any predetermined vertical lines on the
        density plots, such as mean, median, or mode.
        The default is None.
    fileType : str, optional
        The filetype of the image. The default is "png".
    figsize : tuple, optional
        The figure size (see matplotlib documentation for more details).
        The default is (15, 12).

    Returns
    -------
    fig, ax : tuple
        The figure and axes components of the plot.

```{python, plotDensity, eval=FALSE}
plotDensity(chainName, smoothing=0.05, write=True,
            pdir="./Plots/", vlines=None, fileType="jpg",
            figsize=(15, 12))
```

#### plotACF

`plotACF` plots the auto-correlation of the MCMC chain. It can calculated using partial auto-correlation as well, using a Yule-Walker method. 

The author tried to make this function look as much like the R `acf` function as possible, and he found that when $lw = 1 - e^{-0.00346103(s-1)}$, where $lw$ is line width and $s$ is the number of auto-correlation samples, the plot matches the look and feel of the R acf plot quite closely. Surely, there was a better way to make this plot, but it was a fun exercise for the author to find the function that optimized the line width based on the number of samples provided.

    Parameters
    ----------
    chainName : str
        The name of the MCMC chain.
    bounds : bool, optional
        Draw the bounds of the autocorrelation. The default is True.
    ci : float, optional
        The size of the bounds (confidence interval), if applicable.
        The default is 0.95.
    acfType : str, optional
        The type of acf plot to draw. Can be either 'acf' or 'pacf'.
        The default is "acf".
    write : bool, optional
        Write plot to a directory. The default is False.
    display : bool, optional
        Display the plot.
    pdir : str, optional
        The directory to write the plots to. The default is "./Plots/".
    fileType : str, optional
        The filetype of the image. The default is "png".
    lw : float, optional
        The line width to use on the plot. If None, it will be calculated
        automatically. The default is None.
    figsize : tuple, optional
        The figure size (see matplotlib documentation for more details).
        The default is (15, 12).

    Returns
    -------
    fig, ax : tuple
        The figure and axes components of the plot.

```{python, plotACF, eval=FALSE}
plotDensity(chainName, bounds=True, ci=0.95, acfType="acf",
            write=False, pdir="./Plots/", fileType="jpg", lw=None,
            figsize=(15, 12))
```

#### removeChain


`removeChain` searches for a chain labeled `chainName` and removes it from the `chain` attribute/dictionary.


    Parameters
    ----------
    chainName : str
        Chain to remove.
    print_ : bool, optional
        Print a message displaying what was dropped. The default is True.

    Returns
    -------
    None.

```{python, removeChain, eval=FALSE}
removeChain(chainName)
```

#### showChain

This method may be redundant. It prints the chain to the console using the python print method. It has the same effect as calling `mcmcObject.chains['chainName']`, except that the chain can't be stored from the `showChain` method.

Display a chain from the collection.

    Parameters
    ----------
    chainName : str
        The name of the chain from the collection to display.

    Returns
    -------
    None.

```{python, showChain, eval=FALSE}
showChain(chainName)
```

#### TJ_Convergence_test

This is a homemade convergence test for MCMC chains developed by Tom Werner and Joel Linford (TJ). While these ideas were novel to them at the time of discovery, it has since been revealed that their methods are related or similar to diagnostics proposed by more noteworthy statisticians, such as Heidelberger and Welch or Gewke. Still, this method has proven to perform well when compared with more well-known methods.

This method uses moving windows (default is 5) to "slide" across the chain (starting from the beginning) and measure the given quantiles of the chain in each window (for example, the top 5% or the lower 5%), and compare that with the equivalent quantile values at the end of the chain. If all 5 windows are within some percent difference (`eps`, percent expressed in decimals) of the ending quantiles, then the chain is assumed to have reached a stationary distribution. `bin_limit` is defined as the percentage of the chain (starting from the beginning) that the windows are allowed to pass through. If any of the windows pass by the `bin_limt`, then the chain is said to have not reached stationarity.

If `window_size=None`, then the window size is determined to be 5% of the total numbers of rows. `slide` is how much the windows move down the chain after each iteration, and `window_space` is the amount of space between windows. `eps` is the tolerance of the percent difference. `quantiles` should be a list of decimals representing the quantiles of interest. 

This method requires some assumptions:

1. That the chain has been run long enough to have already reached stationarity of the target distribution, not stationarity of an arbitrary distribution
    + For example, this could be used after the Raftery diagnostic or after obtaining a sufficient number of Effective samples
2. That the burn-in value is being interpreted as the closest points in the chain to which stationarity occurs, and that the maximum value between parameters is chosen for the burn-in value
3. That both parameters must pass for the burn-in value to be valid and for the chain to be considered reaching stationarity.

    

```{python, TJ_Convergence_test, eval=FALSE}
TJ_Convergence_test(self, chainName, eps=0.025, quantiles=[0.05, 0.95],
                    window_size=None, num_windows=5, slide=50,
                    window_space=0, bin_limit=0.6, print_final=False)
```

### Other Supporting Functions

The following methods are not part of the mcmcObject class. Instead, they are supporting functions that are imported as part of `import tsdst.mcmc`.

#### applyMCMC

Due to some randomness that causes errors when computing the square root of a matrix (using Cholesky decomposition in most cases), this method will run the MCMC algorithms until the computation succeeds. The default is to try 100 times, and jitter the starting values with random decimals every 5 iterations. For example, at every 5th iteration, `st` is jittered by $RV$ where $RV \sim N(0, sd)$, therefore $st := st + RV$. 

    Parameters
    ----------
    st : numpy array
        An array of the parameter starting values.
    ni : int
        NUmber of MCMC iterations.
    lp : function
        Function for the log posterior.
    algo : function
        MCMC algorithm to be performed.
    algoOpts : dict, optional
        Specific options for the MCMC algorithm. The default is None.
    postArgs : dict, optional
        Specific options for the posterior function. The default is None.
    sd : float, optional
        The standard deviation of the normal distribution used to draw the
        jitter amount from. In other words, the jittered covariance is the 
        covaraince matrix plus a random draw X, where X~N(0, sd). 
        The default is 0.02.
    max_tries : int, optional
        The max number of times to try and jitter before admitting defeat.
        If the jitter fails, the reason or the covaraince matrix not being
        positive definite may not be due to randomness, and may require
        a re-evaluation of the problem space. The default is 100.

    Raises
    ------
    ValueError
        Raised when cholesky decomposition fails after max_tries.

    Returns
    -------
    res : tuple
        Returns tuple containing the MCMC results.

```{python, applyMCMC, eval=FALSE}
applyMCMC(st, ni, lp, algo, algoOpts=None, postArgs={},
          sd=0.02, max_tries=100)
```

#### cholupdate

Performs a rank 1 update of a Cholesky decomposition. If `update=False`, then it performs a downdate. It's mainly used in [adaptive_mcmc] to help generate the covariance matrix (see [adaptive_mcmc] for details). An update will add x to L while a downdate will subtract x from L.

    Parameters
    ----------
    L : numpy array (float)
        The upper-triangular decomposed matrix, shape=(N, N).
    x : numpy array (float)
        The values being added to L, shape=(N, ).
    update : bool, optional
        Perform an update (as opposed to a downdate). The default is True.

    Returns
    -------
    L : numpy array
        Return updated L matrix.

```{python, cholupdate, eval=FALSE}
cholupdate(L, x, update=True)
```

#### minESS

Translated from the R mcmcse package.
    
Calculates the minimum Effective Sample Size, independent of the MCMC chain for the given number of parameters. `alpha` is the confidence level, `eps` is the tolerance level (ignored when `ess is not None`), and `ess` is the effective sample size. When `ess is not None`, the function returns the tolerance level needed to obtain that ESS.

In practice, the user should find the minESS amount and then sample until they hit that number. Usually, it is computationally difficult to obtain the optimal minimum effective sample size, therefore, it is useful to know what tolerance is needed to obtain the samples that can be afforded computationally.

see [mulitESS] or [mcmcse::minESS](https://cran.r-project.org/web/packages/mcmcse/mcmcse.pdf) for more information.

    Parameters
    ----------
    p : int
        The dimension of the estimation problem (i.e. the number of parameters
        represented in the MCMC chain, or the number of columns in the MCMC
        chain).
    alpha : float, optional
        Confidence level. The default is 0.05.
    eps : float, optional
        Tolerance level. The smaller the tolerance, the larger the minimum 
        effective samples. The default is 0.05.
    ess : int, optional
        Estimated effective sample size. The default is None.

    Returns
    -------
    int
        The minimum effective sample required for a given eps tolerance.
        If ess is specified, then the value returned is the eps corresponding
        to that ess.

```{python, minESS, eval=FALSE}
minESS(p, alpha=0.05, eps=0.05, ess=None)
```

#### mulitESS

This function computes the Effective Sample Size of an MCMC chain. Due to correlation between MCMC samples, it is sometimes unclear how much information about the parameters has been obtained. If all of the MCMC samples were independent, we would need less samples to get accurate information about the posterior than when the samples are correlated. ESS measures the amount of independent samples that have actually been obtained in the MCMC chain, and mESS is a special case for multivariate posteriors. In other words, this method is a way to test if your chain has gone far enough.

This information can used in conjunction with [minESS], such that the chain has sampled enough when $multiESS >= minESS$.

This code is a translation of the R package `mcmcse`. For more information regarding these functions, see their [documentation](https://cran.r-project.org/web/packages/mcmcse/mcmcse.pdf). 

See [@mESS], [@mESS_2018], or [@mESS_2018] for more technical information.

    Parameters
    ----------
    chain : numpy array
        The MCMC chain, where the rows are samples.
    covmat : numpy array, optional
        The covaraince matrix for the parameters, if available. If None,
        matrix is obtained from mcse_multi. The default is None.
    g : function, optional
        A function that represents features of
        interest. `g` is applied to each row of x, and should thus take a
        vector input only. If g is none, g is set to be identity, which is
        estimation of the mean of the target density. The default is None.
        
        An example of g would be the sum of the second moments of
        each parameter, i.e.:
        
        def g(x):
            return np.sum(x**2)
        
    mcse_multi_args : dict
        Arguments for mcse_multi function. Don't use this if a suitable matrix
        estimate from mcse_multi or mcse_initseq is already obtained. The
        default is an empty dictionary

    Returns
    -------
    ess : int
        The estimated effective sample size.

```{python, multiESS, eval=FALSE}
multiESS(chain, covmat=None, g=None, **args)
```

#### raftery

This function can be called using `tsdst.mcmc.raftery()`. This computes the Raftery-Lewis diagnostic for a single chain, where `q` is the quantile of interest, `r` is the desired accuracy of the quantile of interest, and `s` is the probability. In other words, `s` is saying that if you were to take a sample of millions of quantiles from chains using the same methods, $s$% of the quantiles would reflect the $q^{th}$ quantile of the target distribution to within $\pm r$. 

The return values of the diagnostic are as follows:

* $N$: Required number of samples
    + The minimum chain length needed to estimate a percentile to some precision
* $N_{min}$: Minimum pilot run length
    + The minimum number of samples needed to evaluate the chain
    + Formally: the minimum sample size based on zero autocorrelation
* $M$: Burn-in
    + The number of samples to be discarded from the beginning of the chain (after obtaining $N$ samples)
* $I$: Dependence factor
    + The extent that autocorrelation is inflating $N$
    + $I = \frac{M + N}{N_{min}}$
* $K$: The recommended thinning of the chain (performed after obtaining $N$ samples) 

See [@raft1] or [@raft2] for more information.

    Parameters
    ----------
    chain : numpy array
        MCMC chain.
    q : float, optional
        Quantile of interest (in terms of percentile, i.e. between 0 and 1).
        The default is 0.025.
    r : float, optional
        Accuracy. The default is 0.005.
    s : float, optional
        Probability. The default is 0.95.
    converge_eps : float, optional
        Convergence threshold (epsilon). The default is 0.001.
    thin : int, optional
        Thining amount. The default is 1.
    print_ : bool, optional
        Print results. The default is False.

    Raises
    ------
    ValueError
        Raised if there are not enough samples in the chain, given the q,r,s
        values, or if there is an invalid selection of q.

    Returns
    -------
    None.

```{python, raftery, eval=FALSE}
raftery(chain, q=0.025, r=0.005, s=0.95, converge_eps=0.001,
        thin=1, pr=False)
```

#### samp_size_calc_raftery

This function calculates $N_{min}$ for the Raftery diagnostic (see [raftery]).

    Parameters
    ----------
    q : float, optional
        Quantile of interest (in terms of percentile, i.e. between 0 and 1).
        The default is 0.025.
    r : float, optional
        Accuracy. The default is 0.005.
    s : float, optional
        Probability. The default is 0.95.

    Returns
    -------
    phi : float
        Phi parameter in Raftery evaluation.
    nmin : int
        Minimum number of samples needed.

```{python, samp_size_calc_raftery, eval=FALSE}
samp_size_calc_raftery(q=0.025, r=0.005, s=0.95)
```

### mcmcObject Algorithms

In practice, it is rare that a person knows the covariance matrix to use in sampling from the proposal distribution. Therefore, adaptive methods have begun popular because the algorithms can "learn" the covariance matrix needed. 

#### adaptive_mcmc

A random walk metropolis algorithm that adaptively tunes the covaraince matrix. Based on methods by Rosenthal (who improved on Haario's method). The method by Rosenthal is sometimes refered to as Adaptive Mixture Metropolis, while the algorithm by Haario is called Adaptive Metropolis and is generally considered to be the historically first adaptive Metropolis algorithm.

This method is identical to a Random Walk Metropolis algorithm, except that it adapts the covariance matrix after each iteration based on the sample covariance of the entire chain up to the current iteration. The candidate values in the MCMC chain are then sampled from a mixed density distribution defined as follows:

$$ Q_{n}(x, \cdot) = (1 - \beta)N(x, (2.38)^{2} \Sigma_{n}/d) + \beta N(x, (0.1)^{2} I_{d} / d)$$

* $Q_{n}$: The proposal distribution
* $N$: Normal Distribution
* $\beta$: some small constant, usually $0.05$
* $\Sigma_{n}$: sample covariance up to the $n^{th}$ iteration
* $d$: the number of parameters
* $I_{d}$: the $d \times d$ identity matrix

To sample from any density distribution, you must first calculate the inverse cdf of the function, otherwise known as the quantile function. Assuming that there is a simple method to generate a uniform random number with range $[0, 1]$ (most programming languages do), then calculating a random number from any distribution is simple. First, generate the random uniform number, and use it as the input to the quantile function. However, in many practical situations (such as with the normal distribution), calculating the quantile function is non-trivial. The same is true for the above mixed density distribution. Therefore, to simplify sampling from the mixed density distribution, use a uniform random number generator to sample a number $U$ between $[0, 1]$. Then, if $U < \beta$, then sample the candidate value from $N(x, (2.38)^{2} \Sigma_{n}/d)$. Otherwise, sample from $N(x, (0.1)^{2} I_{d} / d)$. 

To calculate $\Sigma_{n}$ efficiently, the algorithm uses the [cholupdate] function. Let $X$ be an $m \times n$ matrix of MCMC samples. Then, the equation for population covariance can be defined as follows:

$$\Sigma_n = E(XX^T) - E(X)E(X^T) = \frac{XX^{T}}{n} - \mu \mu^{T}$$

The algorithm calculates the cholesky distribution of $XX^{T}$ for the first four samples. From that point on, $XX^{T}$ is updated using [cholupdate] where `update=True`. If $U < \beta$, then $\big( \frac{XX^{T}}{n} - \mu \mu^{T} \big)$ is calculated using `cholupdate(XXT/n, uuT, update=False)`. This algorithm uses the sample covariance, which can be calculated using the following trick:

$$\Big( \frac{XX^{T}}{n} - \mu \mu^{T} \Big) \Big(\frac{n}{n-1} \Big)$$

The algorithm multiplies $\Sigma_{n}$ by $\frac{2.38^2}{d}$, which makes the final covariance used to create the candidate samples:

$$\Sigma_{n} = \Big( \frac{2.38^2}{d} \Big) \Big( \frac{XX^{T}}{n} - \mu \mu^{T} \Big) \Big(\frac{n}{n-1} \Big)$$

The options for this algorithm are passed as a dictionary and are defined as follows:

```{r, adaptive_mcmc_table, echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("beta", "progress", "prev_vals"),
                 c(0.05, TRUE, "{'chol2': None, 'sumx': 0, 'prev_i': 0}"),
                 c("float", "boolean", "dictionary"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

    Parameters
    ----------
    start : numpy array
        Starting values for the MCMC.
    niter : int
        Number of iterations.
    lpost : function
        Log posterior function.
    postArgs : dict
        Extra arguments for the log posterior function. If there are none, pass
        an empty dictionary.
    options : dict, optional
        Extra arguments for the MCMC algorithm, namely:
            beta : float
                Between 0 and 1. Decides the proportion to sample for each
                section of the mixture distribution.
                
                A mixture distribution is essentially like adding two 
                distributions together. However, to avoid some complicated
                math, one way to sample from a mixture of two distributions is
                to use a trick, namely, to first sample from a uniform
                distribution between 0, 1, and then evaluate whether that value
                is above some threshold (beta in this case). If it is, sample 
                from the first distribution, otherwise, sample from the second.
            progress : bool
                Whether to display progress bar
            prev_vals : dict
                The previous values of the last run, namely:
                    chol2 : numpy array
                        the decomposed covariance matrix of the parameters
                    sumx : numpy array
                        the current sum of the parameter value (for each
                                                                parameter) 
                    prev_i : int or float
                        the number of samples represented in sumx.
                        Used in averaging sumx
        The default is None.

    Returns
    -------
    parm : numpy array
        MCMC samples.
    prev_vals : dict
        The ending values of the MCMC algorithm. Useful when you want to
        continue where you left off.

```{python, adaptive_mcmc, eval=FALSE}
adaptive_mcmc(start, niter, lpost, options=None, **args)
```

See [@adapmcmc], [@adapmcmc2], [@adapmcmc3], or [@adapmcmc4] for more information.

#### rwm

This is a basic random walk metropolis algorithm. It requires a covariance matrix to work properly, but if not supplied, it will use $\frac{2.38^{2}}{n}\Sigma_{I_n}$ where $n$ is the number of parameters and $\Sigma$ is the $n \times n$ identity matrix. The options for this algorithm are passed as a dictionary and are defined as follows:

```{r, rwm_table, echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("E", "progress", "prev_vals"),
                 c("((2.38**2)/numParams)*np.diag(np.repeat(1, numParams))", TRUE, "{'E_0': None}"),
                 c("np.array", "boolean", "dictionary"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

    Parameters
    ----------
    start : numpy array
        Starting values for the MCMC.
    niter : int
        Number of iterations.
    lpost : function
        Log posterior function.
    postArgs : dict
        Extra arguments for the log posterior function. The default is
        an empty dictionary.
    options : dict, optional
        Extra arguments for the MCMC algorithm, namely:
            E : numpy array
                The covariance matrix
            progress : bool
                Whether to display progress bar
            prev_vals : dict
                The previous values of the last run, namely:
                    E_0 : numpy array
                        the final covaraince matrix
        The default is None.

    Returns
    -------
    parm : numpy array
        MCMC samples.
    prev_vals : dict
        The ending values of the MCMC algorithm. Useful when you want to
        continue where you left off.

```{python, rwm, eval=FALSE}
rwm(start, niter, lpost, options=None, **args)
```

#### rwm_with_lap

This is a random walk metropolis algorithm with a log-adaptive proposal distribution. It functions similar to [adaptive_mcmc], except that it updates the covariance matrix every $k$ samples. 

The options for this algorithm are passed as a dictionary and are defined as follows:

```{r, rwm_with_lap_table, echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("k", "c_0", "c_1", "progress", "prev_vals"),
                 c(20, 1, 0.8, TRUE, "{'E_0': None, 'sigma_2': None, 't': 0}"),
                 c("int", "float", "float", "boolean", "dictionary"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

    Parameters
    ----------
    start : numpy array
        Starting values for the MCMC.
    niter : int
        Number of iterations.
    lpost : function
        Log posterior function.
    postArgs : dict
        Extra arguments for the log posterior function. The default is
        an empty dictionary.
    options : dict, optional
        Extra arguments for the MCMC algorithm, namely:
            E : numpy array
                The covariance matrix
            progress : bool
                Whether to display progress bar
            prev_vals : dict
                The previous values of the last run, namely:
                    E_0 : numpy array
                        the final covaraince matrix
        The default is None.

    Returns
    -------
    parm : numpy array
        MCMC samples.
    prev_vals : dict
        The ending values of the MCMC algorithm. Useful when you want to
        continue where you left off.

```{python, rwm_with_lap, eval=FALSE}
rwm_with_lap(start, niter, lpost, options=None, **args)
```

## tsdst.metrics

A collection of metrics to be used for various data science functions.

### aicCalc

Generic Akaike information criterion (AIC) calculation for models. Includes metrics for AIC: `aic`, Corrected AIC: `aicc`, Bayesian Information Criterion (BIC): `bic`, or extended BIC: `ebic`.

    Parameters
    ----------
    loglike : float
        The loglikelihood of the model.
    num_model_params : int
        Number of parameters in the model.
    sample_size : int
        Number of observations.
    c : int or float, optional
        correction parameter for ebic. The default is 2.
    metric : str, optional
        The metric to evaluate (see main description). The default is "aicc".

    Raises
    ------
    ValueError
        Raised if no valid metric is selected.

    Returns
    -------
    ic : float
        The IC value.

### adj_r2

Adjusted R-squared for a linear regression model. The percent of the variance in the response that can be explained by the predictors. The adjustment is for the ratio or predictors to observations.

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_pred : numpy array
        Predicted values.
    X : numpy array
        The design or feature matrix.
    rsquared : float
        The Unadjusted R-squared for the model, if available. Otherwise,
        it's calculated from `r2`

    Returns
    -------
    rsquared : float
        R-squared.

### auc_score

Area under the ROC curve. Wrapper for compatibility with sklearn custom scorers.

    Parameters
    ----------
    estimator : sklearn model, or similar
        The model. Needs to have a predict_proba method.
    X : numpy array or pandas dataframe
        The design or feature matrix.
    y : numpy array or pandas series
        The target or response variable.

    Returns
    -------
    auc : float
        The area under the roc curve.

### bias

Calculate model bias (systemic errors in estimation) in Linear Regression.
    
Defined as The average distance (and direction) the predictions are from the true values (mean(y_pred - y_true)). If bias < 0, predictions are too low, and if bias > 0, then predictions are too high. Bias is measured in the same units as the response.

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_pred : numpy array
        Predicted values.

    Returns
    -------
    bias : float
        Model bias.

### conf_mat_metrics

Confusion Matrix metrics for 0/1 class classification. Assumes 1 is the positive class (i.e. 1 == true positive, 0 == true negative).

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_pred : numpy array
        Predicted 0/1 values.
    conf_metric : str, optional
        The metrics to return, can be `Sens/Recall`, `Specificity`, `ppv`,
        `npv`, `conf_mat`, or `all`. The default is 'all'.

    Returns
    -------
    dict, or float
        Returns dictionary for `all`, otherwise, float for the metric of
        interest.

### cox_snell_r2

Cox-Snell Pseudo R-squared.

    Parameters
    ----------
    ll_est : numpy array
        Estimated Log likelihood.
    ll_null : numpy array
        Null-model log-likelihood.
    n_obs : int
        Number of observations.

    Returns
    -------
    cs_r2 : float
        Cox-Snell Pseudo R-squared.

### glm_regularized_AIC

Calculate AIC for a Generalized Linear Model with regularization.
    
See 'AIC for the Lasso in GLMs', Y. Ninomiya and S. Kawano (2016)

    Parameters
    ----------
    X : numpy array or pandas dataframe
        Feature or design matrix.
    Y : numpy array or pandas series
        Target or response variable.
    reg_mod : sklearn, or similar
        The regularized model.
    unreg_mod : sklearn, or similar
        The unregularized model.
    tol : float, optional
        Tolerance cutoff for counting non-zero coefficients. The default is
        1e-6.
    method : str, optional
        The method for calculating the AIC. Either `kawano` or `Hastie`.
        The default is "kawano".
    family : str, optional
        The type of generalised linear model. The default is "binomial".

    Raises
    ------
    ValueError
        Raised if an invalid family is picked.

    Returns
    -------
    aic : float
        The calculated AIC.

### HellingerDistanceMVN

Quantifies the similarity between two multivariate normal distributions.

    Parameters
    ----------
    mu1 : numpy array
        Mean of the first distribution.
    mu2 : numpy array
        Mean of the second distribution.
    cov1 : numpy array
        Covariance of the first distribution.
    cov2 : numpy array
        Covariance of the second distribution.
    squared : bool, optional
        Return the squared distance. The default is False.

    Returns
    -------
    float
        Hellinger Distance.

### HellingerDistanceUN

Quantifies the similarity between two Univariate normal distributions.

    Parameters
    ----------
    mu1 : float
        Mean of the first distribution.
    mu2 : float
        Mean of the second distribution.
    sd1 : float
        Standard Deviation of the first distribution.
    sd2 : float
        Standard Deviation of the second distribution.
    squared : bool, optional
        Return the squared distance. The default is False.

    Returns
    -------
    float
        Hellinger Distance.

### HotellingsTwoSampleMVTtest

Used for multivariate hypothesis testing. Can test the similarity of two Multivariate samples assumed to follow a normal distribution.

    Parameters
    ----------
    mu1 : numpy array
        Mean of the first distribution.
    mu2 : numpy array
        Mean of the second distribution.
    cov1 : numpy array
        Covariance of the first distribution.
    cov2 : numpy array
        Covariance of the second distribution.
    n1 : int
        Number of observations in the first sample.
    n2 : int
        Number of observations in the first sample.
    pval : bool, optional
        Calculate the p-value of the statistic. The default is True.

    Returns
    -------
    float
        Either p-value or t-statistic.

### js_div

Jensen-Shannon Divergence
    
    px: Probability of x (float or array of floats)
    py: Probability of y (float or array of floats)

### lr_se_fromModel

Linear Regression Model Standard deviation (error) estimate, or, the estimate of the true standard deviation of the underlying distribution for y_true. This is the maximum likelihood estimate for sigma, where $y_{true} \sim N(y_{pred}, \sigma^2)$.

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_pred : numpy array
        Predicted values.
    num_params : int
        Number of Model Parameters

    Returns
    -------
    sigma : float
        Model error estimate.

### lr_se_fromPrediction

Linear Regression Model Standard deviation (error) estimate, or, the estimate of the true standard deviation of the underlying distribution for y_true. This is the maximum likelihood estimate for sigma, where $y_{true} \sim N(y_{pred}, \sigma^2)$.

    Parameters
    ----------
    estimator : sklearn model, or similar
        The model. Needs to have a predict_proba method.
    X : numpy array or pandas dataframe
        The design or feature matrix.
    y : numpy array or pandas series
        The target or response variable.

    Returns
    -------
    sigma : float
        Model error estimate.

### mcfadden_r2

McFadden's Pseudo R-squared when the saturated model is not available.

    Parameters
    ----------
    ll_est : numpy array
        Estimated Log likelihood.
    ll_null : numpy array
        Null-model log-likelihood.

    Returns
    -------
    m_r2 : float
        McFadden Pseudo R-squared.

### nagelkerke_r2

Nagelkerke Pseudo R-squared. Provides a correction to Cox-Snell Pseudo R-squared to bound between 0, 1.

    Parameters
    ----------
    ll_null : numpy array
        Null-model log-likelihood.
    n_obs : int
        Number of observations.
    cs_r2 : float
        Cox-Snell Pseudo R-squared. Default is None.
    ll_est : numpy array
        Only needed if cs_r2 not provided. Estimated Log likelihood.
        Default is None.

    Returns
    -------
    cs_r2 : float
        Nagelkerke Pseudo R-squared.
        
### number_of_nonzero_coef

Calculates the number of non-zero coefficients in a model. Useful for evaluating models with an L1 penalty.

    Parameters
    ----------
    X : numpy array or pandas dataframe
        The design or feature matrix.
    model : sklearn object, or similar
        The model, which needs to have a coef_ attribute.

    Returns
    -------
    num_coef : int
        Number of non-zero coefficients.

### r2

R-squared for a linear regression model. The percent of the variance in the response that can be explained by the predictors.

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_pred : numpy array
        Predicted values.

    Returns
    -------
    rsquared : float
        R-squared.

### rpmse

Calculate model RPMSE (root predictive mean squared error) in Linear Regression.
    
    PMSE or MSE is defined as the average squared distance the predictions
    are from the true values (mean((y_pred - y_true)^2)), and measure how far 
    off the predictions are on average (i.e. how variable the predictions are).
    The root MSE is convenient because it is measured in the same units as the
    response.

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_pred : numpy array
        Predicted values.

    Returns
    -------
    bias : float
        Model bias.

### tjur_r2

Tjur Pseudo R-squared

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_score : numpy array
        Predicted probabilities.

    Returns
    -------
    t_r2 : float
        Tjur Pseudo R-squared.

### top_20p

Measures the percent of true positives that are captured in the top 20% of the population (sorted by predicted probabilities).

    Parameters
    ----------
    y_true : numpy array
        True values (observed response).
    y_score : numpy array
        Predicted probabilities.

    Returns
    -------
    top_20 : float
        Percent of true positives that are captured in the top 20% 
        of the population.

### top_20p_score

Measures the percent of true positives that are captured in the top 20% of the population (sorted by predicted probabilities).
    
    For use with custom scorers in sklearn, otherwise, use top_20p.

    Parameters
    ----------
    estimator : sklearn model, or similar
        The model. Needs to have a predict_proba method.
    X : numpy array or pandas dataframe
        The design or feature matrix.
    y : numpy array or pandas series
        The target or response variable.
    yprob : None
        For compatibility with sklearn custom scorers. Default is None.

    Returns
    -------
    top_20 : float
        Percent of true positives that are captured in the top 20% 
        of the population.
        
## tsdst.modeling

A collection of functions that assist in evaluating statistical models.

### beta_trans

Transform the model coefficients in a logistic regression model.

    Parameters
    ----------
    coef : numpy array
        The model coefficients.
    type_ : str, optional
        The type of transform, either percent, odds, or decimal.
        
        Decimal and percent are the same, except that percent = decimal*100
        
        The default is "percent".

    Returns
    -------
    numpy array
        Transformed Coefficients.

### bPCA

An algorithm to compute missing values using Bayesian PCA. Translated from MATLAB code by Shigeyuki OBA, 2002 May. 5th.

    Parameters
    ----------
    data : numpy array
        The data with missing values to be estimated.
    k : int, optional
        The number of components to consider. Must be less than the number of
        columns. If None, use num_cols - 1. The default is None.
    maxepoch : int, optional
        Number of iterations. The default is 200.
    stepsize : int, optional
        The number of iterations to compute before printing results.
        The default is 10.
    dtau_tol : float, optional
        The precision tolerance. Breaks if precision is lower than the
        tolerance. The default is 1e-8.

    Returns
    -------
    M : dict
        A dictionary of the results, containing:
            mu: The estimated mean row vector
            W: The estimated principal axis matrix
            tau: The estimated precision (inverse variance) of the residual 
                 error

```{python, bPCA}
import numpy as np
from tsdst.modeling import bPCA

X = np.array([[1,2,3],
              [4,5,6],
              [7,np.nan,9],
              [10,11,12]])
    
M = bPCA(X, dtau_tol=1e-6, maxepoch=1000, stepsize=10)
print(M['yest'])
```

### crossVal

A custom cross-validation strategy. While sklearn has a lot of beefed up functionality regarding cross-validation strategies, I like this one because it puts most of the strategies I like to use in one place. Also, it allows for you to output and calculate several cross-validation metrics at once.
    
    One thing that still needs to be added here is parallelization, since each
    of the folds can be calculated simultaneously.

    Parameters
    ----------
    X : pandas dataframe
        Feature or design marix.
    Y : pandas series
        Response or Target variable.
    cv_iterations : int
        The number of cross validation iterations. For k-fold, it is the
        number of folds, and for shuffle, it is the number of iterations.
    model : sklearn, or similar
        model to fit to the data. Must have fit, predict, and/or predict_proba
        methods, depending on the metrics.
    method : str, optional
        The method of cross-validation, either k-fold or shuffle.
        The default is 'k-fold'.
    mtype : str, optional
        The type of model, currently either classification or regressor.
        The default is 'classification'.
    stratified : bool, optional
        Use a stratified sample instead of a random sample.
        The default is True.
    print_ : bool, optional
        Print the results along the way. The default is True.
    random_state : int, optional
        Set the random seed for reproducibility. The default is None.
    method_on_X : function, optional
        An optional function to pass that performs an operation on X.
        For example, you could pass a function to perform PCA before
        performing the fits. This would enable the function to be applied to
        each level of split. The method used to transform/modify X can be
        of any class, the only requirement is that it has both fit and
        transform methods, and that the arguments for fit method accept both
        an X and Y argument, even if the Y argument does nothing. You may need
        to create a simple wrapper for this, if you have a known method you 
        want to use, but doesn't quite fit what you need. The default is None.
    mox_args : dict, optional
        Any optional arguments that get passed to the constructor of the
        method_on_X. The default is {}.
    avg : str, optional
        for metrics where it applies, such as AUC, how to calculate the
        metric (see sklearn docs for more details, particularily with AUC).
        The default is "weighted".
    shuffle : bool, optional
        Shuffle the dataset in the k-fold operations. The default is True.
    test_size : float, optional
        A percent representing the size of the test set. The default is 0.1.
    calculate : list or str, optional
        What to calculate the metrics on, either in-sample or out-of-sample,
        or both. In-sample measures the metrics on the train set while
        out-of-sample measures on the test set.
        The default is ['Out of Sample'].
    metrics : list, optional
        The metrics to include in the analysis.
        The default is ['Accuracy', 'F1', 'Sens/Recall',
                        'Specificity', 'ppv', 'npv', 'AUC'].
    Y_for_test_only : pandas series or numpy array, optional
        An alternate target variable to test on, mainly for the use case of
        training on one response (perhaps one that is more informative or
        restrictive), but then predicting on a seperate test set.
        The default is None.
    sample_limit : int, optional
        The minimum acceptable samples in a split for the response
        variable. Only applies to Y_for_test_only currently.
        The default is 20.

    Returns
    -------
    scores : dict
        Dictionary of performance metrics.

```{python, crossVal}
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from timeit import default_timer as dt

from tsdst.modeling import crossVal

X, y = make_classification(n_samples=10000, n_features=50, flip_y=0.2, random_state=42)
X = pd.DataFrame(X, columns=['V' + str(i) for i in range(X.shape[1])])
y = pd.DataFrame(y.reshape(-1, 1), columns=['y'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

sk_mod = LogisticRegression()

results = crossVal(X_train, y_train, 5, sk_mod)
```

### EstimatorSelectionHelper

This is adapted from David Batista, so credit goes to him. 
    
see [here for the original implementation.](davidsbatista.net/blog/2018/02/23/model_optimization)
    
I added some functionality that will extend to the sklearn-deap package as well. see [here for sklearn-deap:](https://github.com/rsteca/sklearn-deap)

    Parameters
    ----------
    models : dict
        A dictionary of model types.
    params : dict
        A dictionary of parameters for each model.
    searchCV : sklearn object
        The type of search being performed.
    searchCVparams : dict
        A dictionary of optional arguments to send to searchCV.

    Raises
    ------
    ValueError
        Raised when a model is missing parameters, or there is a mismatch.

    Returns
    -------
    None.

### getPriors

Calculate the class distributions across each column of the dataset.
    
For classification problems, but could easily be used with regression problems if an idicator variable was made that indicated some split in the data.

    Parameters
    ----------
    data : pandas dataframe
        The feature or design matrix, along with the reponse variable.
    Ycol : str
        The column representing the target or response variable.

    Returns
    -------
    alltab : pandas dataframe
        A dataframe containing all of the results.

### prettyConfMat

Print a pretty confusion matrix.

    Parameters
    ----------
    Ytrue : numpy array or pandas series
        The true class values.
    Ypred : numpy array or pandas series
        The predicted class values.
    print_ : bool, optional
        Print the confusion matrix at the end. The default is True.
    margins : bool, optional
        Include the summed margins in the matrix. The default is True.
    labels : dict, optional
        A dictionary of key-value pairs, where the keys are the labels,
        and the values are the class values in Ytrue. class labels for the
        confusion matrix. The default is None.

    Raises
    ------
    KeyError
        If labels are provided, an error is raised if the size of the array
        containing the labels does not match the number of possible classes.

    Returns
    -------
    confmat : pandas dataframe
        A dataframe containing the confusion matrix.

### printScores

A helper function to print the outputs of the crossVal function.

Currently, it does not print confusion matrices.    

    Parameters
    ----------
    scores : dict
        The dictionary of scores.

    Returns
    -------
    None.

### RegressionSE

Since sklearn doesn't generate the standard errors by default...

    Parameters
    ----------
    X : pandas dataframe
        The feature or design matrix.
    Y : pandas series or numpy array
        The response or target variable.
    fit_mod : sklearn model, or similar
        The fitted model (Logistic or linear Regression).
    logit : bool, optional
        If True, calculate Logistic Regression SE, otherwise, Linear. Default is True
    low_memory : bool, optional
        Sometimes this process uses a lot of memory. If True, calculate using less memory.
        The default is False.

    Returns
    -------
    se : numpy array
        The standard error of the coefficients.

### runScorers

Utility function that performs the cross-validation.

    Parameters
    ----------
    X : pandas dataframe
        Feature or design marix.
    Y : pandas series
        Response or Target variable.
    splits : sklearn.model_selection object
        Contains the split type and information about the split.
    model : sklearn, or similar
        model to fit to the data. Must have fit, predict, and/or predict_proba
        methods, depending on the metrics.
    mtype : str, optional
        The type of model, currently either classification or regressor.
        The default is 'classification'.
    metrics : list, optional
        The metrics to include in the analysis.
        The default is ['Accuracy', 'F1', 'Sens/Recall',
                        'Specificity', 'ppv', 'npv', 'AUC'].
    avg : str, optional
        for metrics where it applies, such as AUC, how to calculate the
        metric (see sklearn docs for more details, particularily with AUC).
        The default is "weighted".
    calculate : list or str, optional
        What to calculate the metrics on, either in-sample or out-of-sample,
        or both. In-sample measures the metrics on the train set while
        out-of-sample measures on the test set.
        The default is ['Out of Sample'].
    method_on_X : function, optional
        An optional function to pass that performs an operation on X.
        For example, you could pass a function to perform PCA before
        performing the fits. This would enable the function to be applied to
        each level of split. The method used to transform/modify X can be
        of any class, the only requirement is that it has both fit and
        transform methods, and that the arguments for fit method accept both
        an X and Y argument, even if the Y argument does nothing. You may need
        to create a simple wrapper for this, if you have a known method you 
        want to use, but doesn't quite fit what you need. The default is None.
    mox_args : dict, optional
        Any optional arguments that get passed to the constructor of the
        method_on_X. The default is {}.
    Y_for_test_only : pandas series or numpy array, optional
        An alternate target variable to test on, mainly for the use case of
        training on one response (perhaps one that is more informative or
        restrictive), but then predicting on a seperate test set.
        The default is None.
    sample_limit : int, optional
        The minimum acceptable samples in a split for the response
        variable. Only applies to Y_for_test_only currently.
        The default is 20.

    Returns
    -------
    keys : dict
        A dictionary containing the results.

### scoreModel

Score a model using the given metrics.

    Parameters
    ----------
    X : pandas dataframe
        Feature or design marix.
    Y : pandas series
        Response or Target variable.
    model : sklearn, or similar
        model to fit to the data. Must have fit, predict, and/or predict_proba
        methods, depending on the metrics.
    metrics : list, optional
        The metrics to include in the analysis.
        The default is ['Accuracy', 'F1', 'Sens/Recall',
                        'Specificity', 'ppv', 'npv', 'AUC'].
    thres : float, optional
        The threshold to use for classification metrics where it might apply.
        If threshold is None, the default predict mehtod is used (which
        for most models uses a boundary at or equivalent to 0.5 probability).
        The default is None.
    mtype : str, optional
        The type of model, currently either classification or regressor.
        The default is 'classification'.
    print_ : bool, optional
        Print progress as applicable. The default is True.
    avg : str, optional
        for metrics where it applies, such as AUC, how to calculate the
        metric (see sklearn docs for more details, particularily with AUC).
        The default is "weighted".

    Returns
    -------
    res : dict
        A dictionary containing the results for each metric.

### vif

Calculate Variance Inflation Factors. The sqrt of vif indicates how many times larger the standard error is than it would be if that variable had no correlation with the other variables 
    
    Expects pandas.DataFrame or numpy.array

    Parameters
    ----------
    data : pandas dataframe or numpy array
        The feture or design matrix.
    root : bool, optional
        Return the sqrt of the variance inflation factors. The default is False.

    Returns
    -------
    pandas dataframe
        The VIFs.

## tsdst.nn

A module for experimenting and running neural networks using numpy. For an introduction to neural networks, see [here.](https://tomwerner5.github.io/tsdst/Gradient_Descent_and_Back_Propagation.html)

An example using this module is provided below:

```{python}
import numpy as np
from matplotlib import pyplot as plt
from timeit import default_timer as dt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

from tsdst.metrics import accuracy
from tsdst.utils import one_hot_encode
from tsdst.nn.model import NeuralNetwork

import warnings
warnings.filterwarnings('ignore')

nf = 8
nc = 2
nobs = 1000
X_og, Y_og = make_classification(n_samples=nobs, n_features=nf,
                                 n_informative=2, n_classes=nc,
                                 flip_y=0.1, class_sep=1,
                                 random_state=42)
X_train, X_test, Y_train, Y_test = train_test_split(X_og, Y_og, test_size=0.3,
                                                    random_state=42)

Y_train_oh = one_hot_encode(Y_train)
Y_test_oh = one_hot_encode(Y_test)

Y_train = Y_train.reshape(-1, 1)
Y_test = Y_test.reshape(-1, 1)

model = {
         'hidden0': {'depth': 10,
                     'activation': 'relu',
                     'derivative': 'relu_der',
                     'activation_args': {},
                     'initializer': 'he_uniform',
                     'dropout_keep_prob': 1,
                     'lambda': {'Weight': 0.01,
                               'activity': 0,
                               'bias': 0
                              },
                    'lp_norm': {'Weight': 2,
                               'activity': 2,
                               'bias': 2
                              },
                    'use_batch_norm': False
                     },
         'hidden1': {'depth': 7,
                     'activation': 'selu',
                     'derivative': 'selu_der',
                     'activation_args': {},
                     'initializer': 'he_uniform',
                     'dropout_keep_prob': 1,
                     'lambda': {'Weight': 0.01,
                               'activity': 0,
                               'bias': 0
                              },
                    'lp_norm': {'Weight': 2,
                               'activity': 2,
                               'bias': 2
                              },
                    'use_batch_norm': False
                     },
         'output': {'activation': 'softmax',
                    'activation_args': {},
                    'cost': 'cross_entropy',
                    'cost_args': {},
                    'derivative': 'softmax_cross_entropy_der',
                    'derivative_args': {},
                    'initializer': 'xavier_normal',
                    'evaluation_metric': 'accuracy',
                    'lambda': {'Weight': 0.01,
                               'activity': 0,
                               'bias': 0
                              },
                    'lp_norm': {'Weight': 2,
                               'activity': 2,
                               'bias': 2
                              },
                    'use_batch_norm': False
                    }
         }
         
n_iters = 5000
batch_size = X_train.shape[0]
eval_size = 1000
nn = NeuralNetwork(model=model,
                   eval_size=eval_size,
                   batch_size=batch_size,
                   num_iterations=n_iters,
                   optimizer='adam',
                   optimizer_args={'learning_rate': 0.001,
                                   'beta1': 0.9,
                                   'beta2': 0.999,
                                   'eps': 1e-8},
                   m_scale=1,
                   bn_tol=1e-6,
                   bn_momentum=0,
                   shuffle=False,
                   print_cost=True,
                   random_state=42)

t0 = dt()                
nn = nn.fit(X_train, Y_train_oh)
t1 = dt()

print('tsdst Runtime (s):', t1 - t0)

plt.figure()
plt.title("tsdst Training Cost and Accuracy per Iteration")
plt.xlabel("Iteration")
plt.ylabel("Value")
plt.plot(nn.costs)
plt.plot(nn.metric)
plt.legend(["Cost", "Accuracy"])
```

```{python}
#################################################################
################### Comparison With Keras #######################
#################################################################

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.initializers import RandomUniform, TruncatedNormal
from tensorflow.keras.optimizers import Adam


def baseline_model():
	  # create model
    model = Sequential()
    model.add(Dense(10, input_dim=nf,
                    kernel_regularizer=l2(0.01),
                    kernel_initializer=RandomUniform(minval=-np.sqrt(6/nf),
                                                     maxval=np.sqrt(6/nf),
                                                     seed=42)))
    model.add(Activation('relu'))
    model.add(Dense(7,
                    kernel_regularizer=l2(0.01),
                    kernel_initializer=RandomUniform(minval=-np.sqrt(6/10),
                                                     maxval=np.sqrt(6/10),
                                                     seed=42)))
    model.add(Activation('relu'))
    model.add(Dense(nc,
                    kernel_regularizer=l2(0.01),
                    kernel_initializer=TruncatedNormal(mean=0.0,
                                                       stddev=np.sqrt(2/(nc + 7)),
                                                       seed=42)))
    model.add(Activation('softmax'))
    
	  # Compile model
    model.compile(loss='categorical_crossentropy',
                  optimizer=Adam(epsilon=1e-8),
                  metrics=['accuracy']
                 )
    return model


k_model = baseline_model()
t2 = dt()
history = k_model.fit(X_train, Y_train_oh, epochs=5000, batch_size=X_train.shape[0],
                    verbose=0, shuffle=False)
t3 = dt()
print('Keras Runtime (s):', t3 - t2)

h_test = nn.predict(X_test)
print("NN Test Accuracy: ", accuracy(Y_test_oh, h_test))
print("Keras Test Accuracy: ", k_model.evaluate(X_test, Y_test_oh,
                                                verbose=0)[1])

glm = LogisticRegression(C=1, max_iter=100000, multi_class='auto', tol=1e-8).fit(X_train, Y_train.reshape(-1, ))
print("GLM Test Accuracy: ", glm.score(X_test, Y_test.reshape(-1, 1)))

plt.figure()
plt.title("Training Cost per Iteration")
plt.xlabel("Iteration")
plt.ylabel("Value")
plt.plot(nn.costs)
plt.plot(history.history['loss'])
plt.legend(["tsdst", "Keras"])
plt.show()

plt.figure()
plt.title("Training Accuracy per Iteration")
plt.xlabel("Iteration")
plt.ylabel("Value")
plt.plot(nn.metric)
plt.plot(history.history['accuracy'])
plt.legend(["tsdst", "Keras"])
plt.show()
```

### activations

A module for activation functions (and their derivatives) related to neural network layers. Eventually, each function will get it's own section, but for now, I will just list them. The sub bullets are the functions that can be passed as strings to the `activation`, `cost`, or`derivative` keys of the model dictionary. The main bullets are the model dictionary keys that the functions can (or rather, should) be used with. 

* cost (for the output layer only)
    * cross_entropy
    * cross_entropy_binary
    * mse
* activation
    * elu
    * relu (using the alpha parameter, this is also leaky relu)
    * selu
    * gelu
    * gelu_approx (a quicker approximation of gelu, accurate to ~1e-5)
    * gelu_speedy (very approximate, very speedy)
    * sigmoid
    * softmax
* derivative
    * elu_der
    * mse_linear_der
    * relu_der
    * selu_der
    * gelu_der
    * gelu_speedy_der (derivative for gelu_speedy)
    * softmax_cross_entropy_der

The following exist as reference, but are not meant to be used in the model dictionary.

* cross_entropy_der
* cross_entropy_binary_der
* linear_der
* mse_der
* sigmoid_der
* softmax_cross_entropy_der_fullmath
* softmax_der
* softmax_oneout

### initializers

Common functions to initialize layers in Neural Networks. These can be passed as strings to the `initializer` key in the model dictionary.

* he_normal
* he_uniform
* lecun_normal
* lecun_uniform
* random
* xavier_normal (also known as glorot normal)
* xavier_uniform (also known as glorot uniform)

### model

#### NeuralNetwork

This is a class for instantiating and running a neural network. It has the same form as an sklearn model. 

##### __init_\_

The constructor for the NeuralNetwork class.

    Parameters
    ----------
    model : dict
        A dictionary containing the model components, layers, and other
        specifications. The dictionary should have the following general
        structure:
            {
             'hidden0': {'depth': 10,
                         'activation': 'relu',
                         'derivative': 'relu_der',
                         'activation_args': {},
                         'initializer': 'he_uniform',
                         'dropout_keep_prob': 1,
                         'lambda': {'Weight': 0,
                                   'activity': 0,
                                   'bias': 0
                                  },
                        'lp_norm': {'Weight': 2,
                                   'activity': 2,
                                   'bias': 2
                                  },
                        'use_batch_norm': False
                         },
             'output': {'activation': 'softmax',
                        'activation_args': {},
                        'cost': 'cross_entropy',
                        'cost_args': {},
                        'derivative': 'softmax_cross_entropy_der',
                        'derivative_args': {},
                        'initializer': 'xavier_normal',
                        'evaluation_metric': 'accuracy',
                        'lambda': {'Weight': 0,
                                   'activity': 0,
                                   'bias': 0
                                  },
                        'lp_norm': {'Weight': 2,
                                   'activity': 2,
                                   'bias': 2
                                  },
                        'use_batch_norm': False
                        }
             }
        Each layer should have the components defined above, however, not
        every component needs to be used (for example, setting
        dropout_keep_prob = 1 disables dropout). There can be as many
        hidden layers as desired (including none). Simply copy the
        'hidden1' sub-dictionary before the output layer to add a new
        hidden layer. However, the network must have an output layer
        defined. The key names for the layers can be anything, but the
        output layer must be positioned last.
        
        A description of each layer key is defined below:

            activation (str or function): The activation function to be
                                          used. If custom function, it
                                          will pass the affine
                                          transformation of the current 
                                          layer as the first input to the
                                          function.
            activation_args (dict) : An optional dictionary for passing
                                     additional arguments to the activation
                                     or derivative function. If there are
                                     none to pass, use an empty dictionary.
                                     For hidden layers, the derivative and
                                     activation arguments should be the
                                     same, so they share this dictionary.
            cost (str or function): The cost function to be
                                    used. If custom function, it
                                    will pass the true Y values and
                                    the predicted Y values as the first
                                    two inputs to the function.
            cost_args (dict) : An optional dictionary for passing
                               additional arguments to the 
                               cost function. If there are
                               none to pass, use an empty dictionary.
                               Only applies ot the output layer.
            depth (int): The number of hidden nodes in the layer
            derivative (str or function): The derivative of the combined
                                          cost and output layer activation
                                          function to be
                                          used. If custom function, it
                                          will pass the true Y values,
                                          the predicted Y values, and the
                                          non-activated output layer values
                                          as the first inputs to the 
                                          function.
            derivative_args (dict) : An optional dictionary for passing
                                     additional arguments to the derivative
                                     function. If there are none to pass,
                                     use an empty dictionary. This only
                                     applies to the output layer.
            dropout_keep_prob (float) : The proportion of nodes to keep at
                                        the respective layer. Between 0
                                        and 1. If dropping 10% of the
                                        nodes, the keep prob is 0.9
            evaluation_metric (str or function) : An additional evaluation
                                                  metric to be used in 
                                                  training. This is only
                                                  used for printing an
                                                  additional output along
                                                  with cost at each epoch
                                                  or specified iteration to
                                                  track the training
                                                  progress
            initializer (str or function) : The function to be used in 
                                            initializing the layer weights
                                            and biases. If custom, it must
                                            accept two arguments,  
                                            'incoming' and 'outgoing',
                                            which represent how many inputs
                                            are recieved from the previous
                                            layer, and how many outputs
                                            will be calculated at the 
                                            current layer.
            lambda (dict) : A dictionary containing the regularization 
                            penalties for each type of regularization.
                            The options are:
                                Weight (float) : The kernel or weight
                                                 regularizer
                                                 (recommended for use)
                                activity (float) : A regularizer placed on
                                                   the activation function
                                                   output (experimental in
                                                   this code, not
                                                   recommended for use)
                                bias (float) : A regularizer for the bias
                                               (not recommended for use for
                                               theoretical reasons, but
                                               should be correct to use)
                            
                            A value of zero for any of the lambdas will
                            that regularization type for that layers.
            lp_norm (dict) : A dictionary containing the regularization 
                             norm funcitons for each type of
                             regularization.
                             
                             The options are:
                                Weight (int) : The lp-norm for the weight
                                               or kernel regularizer
                                activity (int) : The lp-norm for the
                                                 activity regularizer
                                bias (int) : The lp-norm for the bias
                                             regularizer
            use_batch_norm (bool) : If true, perform batch normalization
                                    on the current layer. For this
                                    implementation, the batch norm layer
                                    is placed before the activation
                                    function and before dropout (if used
                                    together)
            
    eval_size : int, optional
        The number of model evaluations to perform before printing
        an output. It is recommended that this number be
        `int(n/batch_size) + sum([n % batch_size != 0])` where n is the
        number of observations
    batch_size : int, optional
        The number of observations used to update the model at each step.
        The default is 64.
    num_iterations : int, optional
        The total number of full passes through the data to perform
        (i.e. the number of epochs). The default is 500.
    optimizer : str, optional
        The type of optimizer to use for gradient descent.
        The default is 'adam'.
    optimizer_args : dict, optional
        Optional arguments to send to the optimizer (learning rate, etc.). 
        The default is {'learning_rate': 0.001,
                        'beta1': 0.9,
                        'beta2': 0.999,
                        'eps': 1e-8}.
    m_scale : float, optional
        An optional scaling paramter to scale up or down the cost and
        gradient values. The default is 1.
    bn_tol : float, optional
        The tolerance used in the batch norm equations.
        The default is 1e-9.
    bn_momentum : float, optional
        The momentum used in the exponential moving average for the mean
        and variance of the batch norm process. The default is 0.
    scorer : str or function, optional
        The function used in the score method. If custom, it must accept
        the true Y values and the predicted y values as the first two
        arguments of the function.
        The default is 'accuracy'.
    shuffle : bool, optional
        Shuffle the training set before training. The default is False.
    print_cost : bool, optional
        Print the cost (and possibly another metric) at each eval_step.
        The default is True.
    random_state : int, optional
        The random state of the process (for reproducibility).
        The default is 42.

    Returns
    -------
    None.

##### backward_prop

The backward propagation step.

    Parameters
    ----------
    X : numpy array
        The input data.
    Y : numpy array
        The true Y values.
    wb : dict
        A dictionary of the weights/biases for each layer.
    zs : dict
        A dictionary of the linearly (affine transform) activated values
        for each layer.
    batch_norm : dict
        A dictionary containing the initial (or previous) batch norm
        results.
    hzs : dict
        A dictionary of the fully activated values for each layer
    dropout : dict
        A dictionary of the dropout status for each layer.

    Returns
    -------
    dwdb : dict
        A dictionary of the gradients with respect to the weights and
        biases.

##### draw_predicted_samples

Draws predictive samples for each observation from the distribution created by using dropout.

The rows of the output represent the observations in X, and the columns of the output represent the number of samples drawn. If Y has multiple outputs or classes for a single observation, then the shape of the output will be (X.shape[0], Y.shape[1]*n_samples). For example, if there are four observations in X, two classes in Y, and you want three samples, then the output will be:

$$
\left[
{
  \begin{array}{cccccc}

    Y_{11_{c1}} & Y_{11_{c2}} & Y_{12_{c1}} & Y_{12_{c2}} & Y_{13_{c1}} & Y_{13_{c2}} \\
    Y_{21_{c1}} & Y_{21_{c2}} & Y_{22_{c1}} & Y_{22_{c2}} & Y_{23_{c1}} & Y_{23_{c2}} \\
    Y_{31_{c1}} & Y_{31_{c2}} & Y_{32_{c1}} & Y_{32_{c2}} & Y_{33_{c1}} & Y_{33_{c2}} \\
    Y_{41_{c1}} & Y_{41_{c2}} & Y_{42_{c1}} & Y_{42_{c2}} & Y_{43_{c1}} & Y_{43_{c2}} \\

  \end{array}
}
\right]
$$  

    Parameters
    ----------
    X : numpy array
        The input data.
    n_samples : int, optional
        The number of samples to draw. The default is 1000.
    n_outputs : int, optional
        The number of outputs or classes in Y. The default is 1.

    Returns
    -------
    draws : numpy array
        The sampled predicted values.

##### fit

Fits the Neural Network.

    Parameters
    ----------
    X : numpy array
        The input data.
    Y : numpy array
        The true Y values.

    Returns
    -------
    self
        The fitted model.

##### forward_prop

The forward propagation step

    Parameters
    ----------
    X : numpy array
        The input data.
    wb : dict
        A dictionary of the weights/biases for each layer.
    batch_norm : dict
        A dictionary containing the initial (or previous) batch norm
        results.
    train : bool, optional
        Whether the forward propagation method is being used to train or
        calculate the network in it's current state.
        The default is True.
    sample : bool, optional
        Whether or not the forward propagation is being used to generate
        random smaples of the output from the distribution created using
        dropout. The default is False.

    Returns
    -------
    hz : numpy array
        The final predicted output.
    zs : dict
        A dictionary of the linearly (affine transform) activated values
        for each layer.
    batch_norm : dict
        A dictionary containing the initial (or previous) batch norm
        results.
    hzs : dict
        A dictionary of the fully activated values for each layer
    dropout : dict
        A dictionary of the dropout status for each layer.
    regularizers : dict
        A dictionary of the regularization status for each layer.

##### initialize_wb

Initialize the network.

    Parameters
    ----------
    X : numpy array
        The input data.

    Returns
    -------
    wb_list : dict
        A dictionary of the weights/biases for each layer.

##### predict

Predict the Y values based on the input X.

    Parameters
    ----------
    X : numpy array
        The input data.

    Returns
    -------
    hz : numpy array
        The predicted Y values.
        
##### score

A method to score/test the model based on the inputs X, y. This can be a custom function or called with a string from something built-in.

    Parameters
    ----------
    X : numpy array
        The input data.
    y : numpy array
        The true Y values.

    Returns
    -------
    float or numpy array
        The score result for X, y.

### optimizers

The following list contains the current optimizers available (passed as the `optimizer` argument in the class constructor).

* adam
* gd (gradient descent)

## tsdst.optimization

Contains functions for optimization.

### AdaptiveNMorBrent

An adaptive Nelder-Mead method for finding the max/min of functions. This is good for high-dimensional optimization problems. This is similar to scipy.optimize.minimize (see [@scipymin]). The default behavior is minimization. For maximization, pass the negative of `fun`. In one dimension, it performs the Brent method (see [@AdapNM] and [@brent]).

This function is notably slower than scipy.optimize.minimize. It was originally created because the author was having issues passing certain types of arguments to the scipy version of the function.

The options for this algorithm are passed as a dictionary and are defined as follows:

```{r, AdaptiveNMorBrent_table, echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("dom_tol", "max_iter", "bounds", "fun_tol"),
                 c(formatC(1e-8, format = "e", digits = 2), 10000, "[-10000.0, 10000.0]", formatC(1e-8, format = "e", digits = 2)),
                 c("float", "int", "list(float), np.array(float), tuple(float)", "float"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

Note that in the Brent method, `fun_tol` is not used, and in the NM method, `bounds` is not used.

    Parameters
    ----------
    fun : function
        The function to find the roots of.
    start : numpy array
        The starting values for the optimization. Must be equal in length to
        the number of parameters being optimized.
    opt : dict, optional
        Optional arguments for the optimization. The default is None.
    print_ : bool, optional
        Print Results. The default is True.
    fun_args : dict
        Optional arguments to pass to fun.

    Raises
    ------
    ValueError
        Raised if there are invalid bounds, or no valid root.
    TypeError
        Raised if bounds is not a valid type.

    Returns
    -------
    final_res : dict
        A dictionary containing the results.

```{python, AdaptiveNMorBrent}
import numpy as np
from tsdst.optimization import AdaptiveNMorBrent


def gaussian(params, data):
    '''
    A gaussian likelihood.
    
    Parameters
    ----------
    params : numpy array
        The parameters of the distribution to be sampled in mcmc.
    data : numpy array
        The observed data.
        
    Returns
    -------
    like : float
        Negative Likelihood density.
    '''
    mu = params[0]
    sigma = params[1]
    
    like = np.sum((-0.5*np.log(2*np.pi) - np.log(sigma) - 0.5*((data - mu) / sigma)**2))
    
    return -like


X = np.random.normal(size=(100,), loc=100, scale=10)
start = np.random.uniform(size=(2,))

res = AdaptiveNMorBrent(start=start, fun=gaussian, opt=None, print_=True,
                        fun_args={'data': X})
```

### Brent_1DRoot

Evaluates the roots of a function using Brent's method (see [@brent]). This function is meant to be used with the [signIntervals] function.

The options for this algorithm are passed as a dictionary and are defined as follows:

```{r, Brent_1DRoot table, echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("dom_tol", "max_iter", "bounds", "fun_tol"),
                 c(formatC(1e-8, format = "e", digits = 2), 10000, "[-10000.0, 10000.0]", formatC(1e-8, format = "e", digits = 2)),
                 c("float", "int", "list(float), np.array(float), tuple(float)", "float"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

Example:

```{python, Brent_1DRoot, eval=TRUE}
from tsdst.optimization import signIntervals, Brent_1DRoot

def func(x):
  return x**2 - 5

bds = signIntervals(func)
for i in bds:
    Brent_1DRoot(func, opt={'bounds': i})
```

### max_like

`max_like` evaluates the maximum likelihood of a numeric function. It also computes the standard errors from the observed fisher information criteria. It uses [AdaptiveNMorBrent] as the optimization function. It returns the MLE of the parameters, the standard error for each parameter, and a matrix representing the confidence intervals of the parameters (The author can't remember if the rows or the columns separate the parameters...).

The options for this algorithm are passed as a dictionary and are defined as follows:

```{r, max_like_table, echo=FALSE, warning=FALSE}
library("knitr")
mcmcObjTable <- data.frame(c("dom_tol", "maxiter"),
                 c(formatC(1e-8, format = "e", digits = 2), 500),
                 c("float", "int"))
names(mcmcObjTable) <- c("Option", "Default Value", "Type")
kable(mcmcObjTable)
```

```{python, max_like}
import numpy as np
from tsdst.optimization import max_like


def gaussian(params, data):
    '''
    A gaussian likelihood.
    
    Parameters
    ----------
    params : numpy array
        The parameters of the distribution to be sampled in mcmc.
    data : numpy array
        The observed data.
        
    Returns
    -------
    like : float
        Negative Likelihood density.
    '''
    mu = params[0]
    sigma = params[1]
    
    like = np.sum((-0.5*np.log(2*np.pi) - np.log(sigma) - 0.5*((data - mu) / sigma)**2))
    
    return -like


X = np.random.normal(size=(100,), loc=100, scale=10)
start = np.random.uniform(size=(2,))

res = max_like(start=start, lf=gaussian, opt=None, print_=False,
               like_args={'data': X})

print('MLE Estimates: ', res[0])
print('Standard Error: ', res[1])
print('95% CI: ', res[2])
```

### signIntervals

This returns a list of tuples, where each tuple is a range of values that have the same sign. This helps [Brent_1DRoot] find the points where $f(x) = 0$

    Parameters
    ----------
    fun : function
        The function to find the roots of.
    lower : float, optional
        The minimum value to investigate. The default is -50.
    upper : float, optional
        The maximum value to investigate. The default is 50.
    step : float, optional
        The interval between values in (lower, upper) to investigate.
        The default is 0.01.
    fun_args : dict
        Optional arguments to pass to fun.

    Returns
    -------
    bounds : list
        A list of tuples containing the upper and lower bounds of possible
        roots.

```{python, signIntervals, eval=FALSE}
signIntervals(fun, lower=-50, upper=50, step=0.01, *args)
```

## tsdst.parallel

Methods for parallel programming

### p_prog_simp

Parallel Progress bar.

Note: it is a known bug for this function to fail while using sypder. I have not been able to discover why, or provide a fix. If you run a script with this function from the console (or possibly jupyter), it should run just fine. 
    
It is also known that using threads may be buggy for this function. It was intended to be used only on processes, but I included the thread option as a supplement to experiment with later. It is not guarenteed to work.

    Parameters
    ----------
    args : dict
        Dictionary of stationary arguments (i.e. the arguments that are
        constant throughout the loop).
    loop_args : list
        A list of dictionaries that contain the arguments that may change at 
        any point during the loop (this is the iterator, and any additional
        arguments that may have different values at different stages of the
        loop).
    function : function
        The function being being processed in the loop (args and loop args will
        be passed to this function).
    n_jobs : int, optional
        The number of parallel processes to run. The default is 2.

    Returns
    -------
    list
        Returns a list of the function outputs for each iteration.

```{python, p_prog_simp}
import numpy as np
from tsdst.parallel import p_prog_simp

def superpower(x, a, b):
    return x**a - x**b

X = np.random.normal(size=(10,))
a = 3
b = 2

args = {'a': a, 'b': b}
loop_args = [{'x': xx} for xx in X]
p_prog_simp(args=args, loop_args=loop_args, function=superpower, n_jobs=2,
            verbose=True, use_threads=False)
```

### p_prog_simp_memorySafe

Parallel Progress bar. The same as p_prog_simp, except it is more explicit in it's definition. This function was developed to try and limit memory consumption for parallel operations. I'm not 100% sure if it accomplishes that... More testing is needed.

    Parameters
    ----------
    args : dict
        Dictionary of stationary arguments (i.e. the arguments that are
        constant throughout the loop).
    loop_args : list
        A list of dictionaries that contain the arguments that may change at 
        any point during the loop (this is the iterator, and any additional
        arguments that may have different values at different stages of the
        loop).
    function : function
        The function being being processed in the loop (args and loop args will
        be passed to this function).
    n_jobs : int, optional
        The number of parallel processes to run. The default is 2.

    Returns
    -------
    list
        Returns a list of the function outputs for each iteration.

## tsdst.quick_analysis

### QuickAnalysis

A class for performing a quick sweep of the data. Helps establish a baseline model to get started with, as well as quickly identify potentially useful features.

#### initialize

```{python, qa_init, eval=FALSE}
QuickAnalysis(train, holdout, target_var, low_memory=False,
              name="QuickAnalysis")
```

#### RunFullAnalysis

```{python, QuickAnalysis}

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from timeit import default_timer as dt

from tsdst.quick_analysis import QuickAnalysis

X, y = make_classification(n_samples=10000, n_features=150, flip_y=0.2,
                           n_informative=40, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
XY_train = pd.concat((pd.DataFrame(X_train, columns=['V' + str(i) for i in range(X.shape[1])]),
                      pd.DataFrame(y_train.reshape(-1, 1),
                                   columns=['y'])),
                      axis=1)
XY_hold = pd.concat((pd.DataFrame(X_test, columns=['V' + str(i) for i in range(X.shape[1])]),
                      pd.DataFrame(y_test.reshape(-1, 1),
                                   columns=['y'])),
                      axis=1)

qa = QuickAnalysis(train=XY_train, holdout=XY_hold, target_var='y', low_memory=False,
                   name="QuickAnalysis")

qa.RunFullAnalysis(freshStart=True, downsample=False, dropCols=None,
                   dropCor=True, corr_cutoff=0.9, dropVar=True,
                   dropVarTol=0.001, cv_iterations=5,
                   random_state=123, default_penalty=0.01, verbose=False,
                   try_parallel=False, for_stride=2, reduce_features_by=30,
                   num_cs=100, red_metric='bic', red_log_low=-5,
                   red_log_high=0, n_jobs=1, remove_msg=True,
                   chunk=False)
```

## tsdst.sampling

Methods for sampling datasets.

### downSample

In a low base-rate classification problem, it is sometimes advantageous to downsample the majority class until the classes are balanced (or close enough). This is done by randomly removing a proportion of observations from only the majority class.

    Parameters
    ----------
    data : pandas dataframe
        The design or feature matrix (with response).
    target_var : str
        The target or reponse variable.
    majority : int, optional
        Value of the majority class. The default is 0.
    minority : int, optional
        Value of the minority class. The default is 1.
    bal : float, optional
        The class balance after downsampling. The default is 0.5.
    random_state : int, optional
        The random seed for the process. The default is 123.

    Returns
    -------
    data_sub : pandas dataframe
        The downsampled data.

```{python, downSample}
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification

from tsdst.sampling import downSample


X, y = make_classification(n_samples=1000, n_features=8, flip_y=0.2, random_state=42)

XY = pd.concat((pd.DataFrame(X, columns=['V' + str(i) for i in range(X.shape[1])]),
                      pd.DataFrame(y.reshape(-1, 1),
                                   columns=['y'])),
                      axis=1)

# Create artificial minority class for example
XY_0 = XY[XY['y'] == 0]
XY_1 = XY[XY['y'] == 1].loc[np.random.choice(XY[XY['y'] == 1].index, 10), :]

XY_n = pd.concat((XY_0, XY_1))
                     
XY_ds = downSample(XY_n, target_var='y')
```

### genRandSampFromDF

Creates a Random sample of observations (rows) from a Dataframe (data). If `replace=True`, this function can be used for bootstrap samples. 

    Parameters
    ----------
    data : pandas dataframe
        The design or feature matrix (with response).
    sampSize : int
        Number of somples to return.
    replace : bool, optional
        Sample with replacement. The default is False.
    random_state : int, optional
        The random seed for the process. The default is None.

    Returns
    -------
    rand : pandas dataframe
        The randomly sampled data.

```{python, genRandSampFromDF}
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification

from tsdst.sampling import genRandSampFromDF


X, y = make_classification(n_samples=1000, n_features=8, flip_y=0.2, random_state=42)

XY = pd.concat((pd.DataFrame(X, columns=['V' + str(i) for i in range(X.shape[1])]),
                      pd.DataFrame(y.reshape(-1, 1),
                                   columns=['y'])),
                      axis=1)
                     
XY_rs = genRandSampFromDF(XY, 100)
```

### latinHypercube1D

This function computes a latin hypercube sample for a 1-dimensional object. This is helpful in reducing the dimension of an MCMC chain (or other dataset) while still maintaining a near-random sample of the target distribution.

In the case of data used for modeling (i.e. $Y \sim X$), The author has found success in reducing the number of samples using this method (where `sort_col=Y_col_location`), while still maintaing overall shape/performance of the model. This is particularily useful in tuning the hyperparameters of models that may take a long time to converge (such as Support Vector Machines).

Setting `stratified=True` ensures that (in classification examples) the class balance remains the same in the sub-sample. Otherwise, you may be slightly under or over representated in one of the classes. In cases where there is not an even division of $\frac{N_{Observations}}{N_{Samples}}$, `bin_placement="random"` will distribute the bins randomly. For example, if I have 13 observations and desire 5 samples, then when `stratified=False`, `bin_location_and_size = [3, 3, 3, 2, 2]`. When  `stratified=True` and `bin_placement="spaced"`, `bin_location_and_size = [3, 2, 3, 2, 3]`. If `bin_placement="random"`, then the location of the bins is chosen at random.

    Parameters
    ----------
    data : numpy array or pandas dataframe
        The design or feature matrix (with response).
    sampleSize : int
        The number of samples to return.
    random_state : int, optional
        The random seed of the process. The default is None.
    shuffle_after : bool, optional
        Shuffle the results after being sampled. The default is True.
    sort_ : bool, optional
        Sort the data (only set to False if sending presorted data).
        The default is True.
    sort_method : str, optional
        Numpy sort method. The default is "quicksort".
    sort_cols : list, optional
        The columns to include in the sorting. The default is None.
    stratified : bool, optional
        Create a stratified sample. The default is True.
    bin_placement : str, optional
        Method for placing the edges on the sampling bins.
        The default is "random".
    verbose : bool, optional
        Print the results of the process. The default is False.

    Raises
    ------
    ValueError
        Raised if invalid bin_plcement is passed.

    Returns
    -------
    LHC : pandas dataframe or numpy array
        The samples.

```{python, latinHypercube1D}
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification

from tsdst.sampling import latinHypercube1D


X, y = make_classification(n_samples=10000, n_features=8, flip_y=0.2, random_state=42)

XY = pd.concat((pd.DataFrame(X, columns=['V' + str(i) for i in range(X.shape[1])]),
                      pd.DataFrame(y.reshape(-1, 1),
                                   columns=['y'])),
                      axis=1)

XY_lhc = latinHypercube1D(XY, sampleSize=1000, random_state=42, shuffle_after=True,
                          sort_=True, sort_method="quicksort", sort_cols=None,
                          stratified=True, bin_placement="random", verbose=False)
```

## tsdst.tmath

Math functions.

### biserial_rank_corr

Biserial Rank Correlation.

    Parameters
    ----------
    x : numpy array
        Values to be evaluated. Either a single group, or an array containing
        both group values if y_is == 'groups'
    y : numpy array
        Values to be evaluated against x, or, if y_is == 'groups',
        then y is a boolean value indicating which group the values
        of x belong to.
    y_is : bool, optional
        Whether or not y is itself a group of values, or an indicator variable
        for identifying the groups within x. The default is 'groups'.

    Returns
    -------
    r : float
        Biserial rank correlation.

```{python, biserial_rank_correlation}
import numpy as np
from tsdst.tmath import biserial_rank_corr

X = np.random.normal(size=100)
Y = np.random.uniform(size=100)

biserial_rank_corr(x=X, y=Y, y_is="not_groups")
```

### corr

Mimics the R cor function to calculate correlations.

    Parameters
    ----------
    x : numpy array or pandas dataframe
        Either a matrix (to calculate correlations across columns) or a 1D
        array to compare with y.
    y : numpy array or pandas series, optional
        A 1D array to compare with a 1D x, or None if x is a matrix.
        The default is None.
    method : str, optional
        Type of correlation to compute. The default is 'pearson'.

    Raises
    ------
    ValueError
        Raised if method provided is invalid.

    Returns
    -------
    numpy array
        Array containing the correlation.

```{python, corr}
import numpy as np
from tsdst.tmath import corr


X = np.array([[1, 2], [1, 5], [6, 2], [1, 3], [4, 4], [6, 7]])

print('Numpy Pearson Calculation: ', np.corrcoef(X, rowvar=False))
print('corr Pearson Calculation: ', corr(X))
print('corr Spearman Calculation: ', corr(X, method='spearman'))
print('corr Kendall Calculation: ', corr(X, method='kendall'))
```

### cov2cor

Translation of R's cov2cor function

    Parameters
    ----------
    V : numpy array
        Covariance matrix.

    Returns
    -------
    R : numpy array
        Correlation matrix.

```{python, cov2cor}
import numpy as np
from tsdst.tmath import cov2cor


X = np.array([[1, 2], [1, 5], [6, 2], [1, 3], [4, 4], [6, 7]])
X_cor = np.corrcoef(X, rowvar=False)
X_cov = np.cov(X, rowvar=False)
X_cor_2 = cov2cor(X_cov)

print('Numpy Correlation Calculation: ', X_cor)
print('cov2cor Calculation: ', X_cor_2)
```

### mahalanobis

Calculates the Mahalanobis distance of a dataframe/matrix. If `produce="squared"`, it returns the squared Mahalanobis distance. If `produce="leverage"`, it returns the leverage statistic. 

The Mahalanobis distance is the distance of a test point from the center of mass divided by the width of the ellipsoid in the direction of the test point. In other words, it is a multidimensional generalization of the Z-score statistic, and represents how many "standard deviations" the test point is from the center (or dimensional means, i.e. for columns $i = 1,2,...,m$, $\pmb\mu_{center} = (\mu_1, \mu_2, ..., \mu_m)$. Mahalanobis distance is related to Euclidean distance, but it better suited for non-standard shapes (i.e. non-normal, non-linear, etc.). This is because Mahalanobis distance accounts for the fact that a set of points may be more variable in one dimension and less variable in another. This means that an extreme value may not be detected using a Euclidean distance measure because the spread of the data is not equal amoung dimensions ($var(X_1) \ne var(X_2)$). Mahalanobis distance uses the covariance matrix in its calculation to acocunt for this, where the covariance matrix contains information about the spread of each variable (i.e. the variance on the diagonals) and about directional similarity between variables, or the tendency for two variables to move in the same direction (i.e. the covariance on the off diagonals). If the covaraince matrix is equal to the identity matrix (and the covariance matrix is substituted and simplified in the equation), then Mahalanobis distance simplifies to the Euclidean distance. If the covariance matrix is diagonal, then the Mahalanobis distance simplifies to a standardized version of the Euclidean distance.

The math is shown below.

Let:

* $n$ be the number of observations
* $D_{n \times 1}$ be the Mahalanobis distance
* $H_{n \times 1}$ be the leverage statistic
* $X_{n \times m}$ be the data of interest
* $\mu_{X}$ be the column means of $X$ 

Then:

$$D = \sqrt{diag((X - \mu_{X}) (cov(X, X)^{-1})(X - \mu_{X})^T)}$$

or, sometimes more efficiently calculated:

$$D = \sqrt{rowsum((X - \mu_{X}) (cov(X, X)^{-1}) * (X - \mu_{X}))}$$

where $* (X - \mu_{X})$ represents element-wise multiplication. $H$, or leverage, can then be derived as follows:

$$H = \frac{D^2}{n - 1} + \frac{1}{n}$$

It may also be useful to note that if the columns of $X$ are centered (i.e. for columns $i = 1,2,...,m$, $X_i := X_i - \mu_{X_i}$), then:

$$\textbf{H} = X (X^{T} X)^{-1}X^T$$

where the diagonal of $\textbf{H}$ ($[\textbf{H}]_{ii} = H_i$) is the leverage statistic for the $i$-th observation. Note the similarities to the least squares solution in linear regression. Also note that centering $X$ is only required since the $X$ in $$\textbf{H} = X (X^{T} X)^{-1}X^T$$ typically contains an intercept column (the corresponding $\beta_0$ column of ones added to $X$ in the regression equation). If this column of ones is missing, then centering is required before calculating leverage from $X$ using the diagonal of $X (X^{T} X)^{-1}X^T$. Otherwise, the two methods for calculating leverage are equivalent. $\textbf{H}$ is sometimes called the hat matrix because it is typically displayed with a hat such that $\hat{y} = \hat{H}y = [X (X^{T} X)^{-1}X^T]y = X\hat{\beta}$, where $\hat{y}$ represents the predicted values in linear regression. $\textbf{H}$ is also called the projection matrix. See [appendix] for more details.

```{python, mahalnobis, eval=FALSE}
mahalanobis(data, produce=None)
```

### mann_whitney

Mann-Whitney sum-rank test.

    Parameters
    ----------
    x : numpy array
        Values to be evaluated. Either a single group, or an array containing
        both group values if y_is == 'groups'
    y : numpy array
        Values to be evaluated against x, or, if y_is == 'groups',
        then y is a boolean value indicating which group the values
        of x belong to.
    y_is : bool, optional
        Whether or not y is itself a group of values, or an indicator variable
        for identifying the groups within x. The default is 'groups'.

    Returns
    -------
    res : dict
        The results of the Mann-Whitney test.

```{python, mann_whitney}
import numpy as np
from tsdst.tmath import mann_whitney

X = np.random.normal(size=10)
Y = np.random.uniform(size=10)

mann_whitney(x=X, y=Y, y_is="not_groups")
```

### mode_histogram

Calculates the mode using data provided from a histogram. Created because calculating mode from KDE estimate can be slow.

    Parameters
    ----------
    data : numpy array
        Array of data to calculate the mode from.
    delta : int, optional
        Used to calculate histogram bin width. The default is 75.
    dataMax_thres : float, optional
        Max value to consider in hisogram calculation (for the right edge).
        The default is 1e9.
    median_thres : float, optional
        Used to limit the center of the distribution, if median is a really 
        high number. The default is None.

    Returns
    -------
    float
        The mode of the array.
    hist : numpy array
        The histogram used to calculate mode.
        
```{python, mode_histogram}
import numpy as np
from timeit import default_timer as dt

from tsdst.tmath import mode_histogram, mode_kde
from tsdst.utils import print_time


X = np.random.normal(size=10000)
t0 = dt()
md_hist = mode_histogram(X)
t1 = dt()
md_kde = mode_kde(X)
t2 = dt()

print_time("mode_histogram vaule: " + str(md_hist) + " , mode_histogram time: ", ts=t0, te=t1, display_realtime=False)
print_time("mode_kde vaule: " + str(md_kde) + " , mode_histogram time: ", ts=t1, te=t2, display_realtime=False)
```

### mode_kde

Calculate mode from gaussian kernel density estimate (using scipy gaussian_pde).

    Parameters
    ----------
    data : numpy array
        Array to calculate mode from.
    kde_args : dict
        A dictionary containing arguments for gaussian_kde (scipy)

    Returns
    -------
    mode : 
        A numeric value, which is the mode.

```{python, mode_kde}
import numpy as np
from timeit import default_timer as dt

from tsdst.tmath import mode_histogram, mode_kde
from tsdst.utils import print_time


X = np.random.normal(size=10000)
t0 = dt()
md_hist = mode_histogram(X)
t1 = dt()
md_kde = mode_kde(X)
t2 = dt()

print_time("mode_histogram vaule: " + str(md_hist) + " , mode_histogram time: ",
           ts=t0, te=t1, display_realtime=False)
print_time("mode_kde vaule: " + str(md_kde) + " , mode_histogram time: ",
           ts=t1, te=t2, display_realtime=False)
```

### norm

Calculate $L_P$ norm of a vector.

When `p = 0`, the function returns the number of elements (sometimes referred to as the $L_0$ norm, though not officially). Otherwise, the norm takes the general form of $||x||_p = (\sum_{i=1}^n|x_{i}|^{p})^{\frac{1}{p}}$.

Regression applications with L2 penalty often dismiss the full norm, particularly in L2 regularization. I'll note here that there is no added benefit to using norms higher than 2 in those cases. Odd norms (L3 and higher) are non-differentiable at the origin and they behave the same as L1. L4 and higher norms have the same local meaning are differentiable, thus providing no additional useful information in regularization.


    Parameters
    ----------
    data : numpy array
        A 1D numpy array of values.
    p : int
        The LP norm specification (1,2,3, ...).
    use_expo : bool, optional
        Use final exponentiation in norm calculation. Particularily for the 
        L2 norm, regularization applications will use the square of the norm,
        so in the L2 case, when use_expo=True, a 1/p exponent is
        added to the final result.

    Returns
    -------
    numeric
        Value of the LP norm on the vector.

    '''
    
### norm_der

Calculates the LP norm derivative of a vector. See [norm](#norm) for more details

    Parameters
    ----------
    data : numpy array
        A 1D numpy array of values.
    p : int
        The LP norm specification (1,2,3, ...).
    use_expo : bool, optional
        Use final exponentiation in norm calculation. Particularily for the 
        L2 norm, regularization applications will use the square of the norm,
        so in the L2 case, when use_expo=True, a 1/p exponent is
        added to the final result.

    Returns
    -------
    numeric
        Value of the LP norm derivative on the vector.

### percentIncrease

Calculates the percent increase between two numbers.

    Parameters
    ----------
    old : numeric
        The old number.
    new : numeric
        The new number.

    Returns
    -------
    numeric
        The percent increase (as a decimal).

```{python, percentIncrease}
from tsdst.tmath import percentIncrease


old = 25
new = 45
percentIncrease(old=old, new=new)
```

### powerset

Create the powerset (all possible combinations) of a list.

    Parameters
    ----------
    iterable : list or list-like
        The collection of items to create a powerset from.

    Returns
    -------
    list of lists
        list containing all possible combinations.

```{python, powerset}
from tsdst.tmath import powerset


mylist = [1,2,3,4]
powerset(mylist)
```

### rank

Rank items in an array. Using the 'first' method, which ranks ties using the order of appearance. For rank functionality similar to R, see [scipy's `rankdata`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html) function (which can be imported from this module for convenience).'

    Parameters
    ----------
    x : numpy array or array-like
        The array of values to be sorted.
    small_rank_is_high_num : bool, optional
        Smallest rank value is the highest/largest number.
        The default is True.
    rank_from_1 : bool, optional
        Use 1 as the top rank rather than 0. The default is True.

    Returns
    -------
    rk : numpy array
        An array containing the ranks.

```{python, rank}
from tsdst.tmath import rank


X = [27, 45, 10, 23, 91, 102]
rank(X)
```

### scaling

A scaling function to scale a single column or vector of data. It can perform min-max scaling and robust scaling of various forms. The values `a, b` convert the resulting array to the scale given by `a` and `b`. For example, in [Min-Max Scaling], the range of scaled values is between 0 and 1, but can easily be adjusted to any other minimum and maximum value by setting `a, b`. `percentile` controls the amount `x` is scaled by when `scale=None` by obtaining an upper and lower value in the range of x. `ddof` is used when scale is provided either `np.var` or `np.std`. A general form of the function is provided below:

$$x := \frac{(b - a)(x - center)}{scale} + a$$

In this function, the default value for `scale` is $UQ - LQ$, where $UQ, LQ$ represent the upper and lower quartiles of x (defined by `percentile`), and the default values are 0 and 100% (or the min and max). The default value for `center` is the lower quartile. To have the function skip either centering or scaling for any configuration, set `center=0` or `scale=1`. 

    Parameters
    ----------
    x : numpy array
        The data.
    a : float, optional
        The minimum value desired. The default is 0.
    b : float, optional
        The maximum value desired. The default is 1.
    percentile : list or array-like, optional
        Provide percentile bounds on values. The default is (0, 100).
    center : float or str, optional
        Either a value representing the centrality measure, or a string
        for the measure to be computed, either mean or median.
        If None, use the minimum of the percentile bounds.
        The default is None.
    scale : float or str, optional
        Either a value representing the dispersion measure, or a string
        for the measure to be computed, either std (standard deviation),
        var (variance), or maxAbs.
        If None, use the absolute difference of the percentile bounds.
        The default is None.
    ddof : int, optional
        Degrees of freedom for the variance/std. dev measure. The default is 0.

    Raises
    ------
    TypeError
        Raised if center is an invalid type.

    Returns
    -------
    scaled : numpy array
        The scaled values.

Common configurations are provided in the following sections.

#### Max Absolute Value

Calculation:

$$x := \frac{x}{\max{|x|}}$$

Function Inputs:

```{python, max_abs_value, eval=FALSE}
scaling(X, center=0, scale="maxAbs")
```

#### Min-Max Scaling

Calculation:

$$x := \frac{x - \min{x}}{\max{x} - \min{x}}$$

Function Inputs:

```{python, min_max, eval=FALSE}
scaling(X)
```

#### Robust Scaler

This scaler can be used with any percentile range (that is equivalent distance from the center on both sides), but the most common is to use the IQR ranges (i.e. median for centering, and 25/75% for the scaling).

Calculation:

$m_x$: measure of center (such as the median)
$UQ, LQ$: upper and lower quartile

$$x := \frac{x - m_{x}}{UQ_x - LQ_x}$$

Function Inputs:

```{python, robust_scaler, eval=FALSE}
scaling(X, center="median", percentile=(25, 75))
```

#### Standard Scaler

Calculation:

$$x := \frac{x - \mu_{x}}{\sigma_x}$$

Function Inputs:

```{python, standard_scaler, eval=FALSE}
scaling(X, center="mean", scale="std")
```

## tsdst.utils

General functions for various tasks.

### check_dir

Check if a directory exists. If it doesn't, create it.

    Parameters
    ----------
    directory : str
        Directory to check.
    make : bool, optional
        Whether or not to create a missing directory. The default is True.
    verbose : bool
        Print results.

    Returns
    -------
    bool
        True if the directory exists.

### dsn_getTable

Load data into python from a sql database. 
    
Requires a DSN or connection string. Uses sqlAlchemy to make the connection, and closes the connection on completion. See [sqlAlchemy](https://docs.sqlalchemy.org/en/13/core/engines.html) for help building connection strings.

    Parameters
    ----------
    sql : str, or list
        A string containing the sql statement to be ran (It should be something
        that returns a table), or a list of sql statements to return multiple
        tables (tables are returned as a list of pandas dataframes, in this
        case).
    user : str
        The username for the database application, if applicable.
    dialect : str
        The database flavor (oracle, mysql, etc.)
    dsn : str, optional
        Can either be a saved DSN from your computer, or a string that
        represents all the relevant information provided in a DSN. 
        It can also be a host:port combination. See sqlalchemy
        documentation for more details.
    pw : str, optional
        The password for the database application that matches the user
        specified, if applicable. If None or '', getpass will ask for a
        password. If there is no password, just leave it blank.
        The default is None.
    create_engine_args : dict
        A dictionary containing arguments to be passed to sqlAlchemy's
        create_engine function.
    read_sql_args : dict
        A dictionary containing arguments to be passed to pandas'
        read_sql function.

    Returns
    -------
    data : pandas dataframe or list
        A dataframe (or list of dataframes) containing the returned data.

### dsn_saveTable

Save pandas dataframe to database.

    Parameters
    ----------
    data : pandas dataframe
        The data to save to a table.
    tableName : str
        The name of the table in the database.
    user : str
        The username for the database application, if applicable.
    dialect : str
        The database flavor (oracle, mysql, etc.)
    dsn : str, optional
        Can either be a saved DSN from your computer, or a string that
        represents all the relevant information provided in a DSN. 
        It can also be a host:port combination. See sqlalchemy
        documentation for more details.
    pw : str, optional
        The password for the database application that matches the user
        specified, if applicable. If None or '', getpass will ask for a
        password. If there is no password, just leave it blank.
        The default is None.
    create_engine_args : dict
        A dictionary containing arguments to be passed to sqlAlchemy's
        create_engine function. Default is {}
    to_sql_args : dict
        A dictionary containing arguments to be passed to pandas'
        read_sql function. Default is {}

    Returns
    -------
    None.

### keyword_search

Searches a list of text arguments and returns a list that includes/excludes the list elements that contain the keywords.
    
    Useful when searching for column names in a large dataframe, for example.

    Parameters
    ----------
    text_list : list
        List of string values.
    include_keywords : list
        List of keywords to search for.
    exclude_keywords : list, optional
        List of keywords that might be similar to include_keywords, but should
        be excluded nonetheless. The default is None.

    Returns
    -------
    found_list : list
        Returns items from the original list that match the keywords.

```{python, keywordSearch}
from tsdst.utils import keywordSearch


text_list = ['hi bye', 'joe', 'snakes', 'ol joe', 'hojo']
keywordSearch(text_list, 'jo')
```

### infer_feature_type

Infer feature type of a column based on it's `dytpe`. The inferred data types are `categorical`, `numeric`, or `date`.
    
    Parameters
    ----------
    X : dataframe or ndarray
        The dataframe containing columns whose datatype needs to be inferred.
    n_unique : int
        The number of unique values to use as a cutoff for numeric columns,
        for example, if the number of unique values for a numeric column is
        greater than n_unique, consider it numeric. Default is None.
    
    Returns:
    --------
    d_type : str
        String representation of the d_type
    
[numpy arrays.dtypes](https://numpy.org/doc/stable/reference/arrays.dtypes.html) reference table:
    
    ? : boolean
    b : signed byte (sometimes interchangeable with boolean, it seems)
    B : unsigned byte
    i : signed integer
    u : unsigned integer
    f : float
    c : complex float
    m : timedelta
    M : datetime
    O : object
    S : zero-terminated bytes
    a : zero-terminated bytes
    U : unicode string
    V : raw data (void)

### move_columns_to_end

Rearrange a pandas dataframe by moving a column (or list of columns) to the end of the dataframe.

    Parameters
    ----------
    data : pandas dataframe
        Data to rearrange.
    col : str or list
        String containing one column to move, or list containing several.

    Returns
    -------
    data : pandas dataframe
        Data with rearranged columns.

### pretty_print_time

This function creates a pretty output (i.e. clock-like format HH:MM:SS.MS) of an arbitrary time (in seconds). Observe:

    Parameters
    ----------
    ts : float
        Start time (or elapsed time if te is None).
    te : float, optional
        End time. The default is None.
    decimals : int, optional
        The number of decimal places to use for tracking miliseconds.
        The default is 0

    Returns
    -------
    pretty : str
        Prettified time.

```{python, pretty_print_time, echo=TRUE}
from tsdst.utils import pretty_print_time

when_i_started = 0
when_i_stopped = 189.54

pretty_print_time(ts=when_i_started, te=when_i_stopped, decimals=3)
```

### print_message_with_time

Print a message with a timestamp. 

    Parameters
    ----------
    msg : str
        The message you want to print.
    ts : float
        Start time in seconds, or elapsed time if te is None.
    te : float, optional
        End time in seconds. The default is None.
    display_realtime : bool, optional
        Display the system (calendar) time as part of the output.
        The default is True.
    backsn : bool, optional
        Add '\\n'. The default is False.
    log : bool, optional
        Save message to a log file. The default is False.
    log_dir : str, optional
        The directory for the log file. The default is "log".
    log_filename : str, optional
        The name of the logfile. The default is "pmwt.log".
    log_args : str, optional
        The read/write specification, for example, wb, w, a, etc.
        The default is "a" for append. See open function in python for more
        details.
    time_first : bool, optional
        Place time at the begininng of the message. The default is False.
    decimals : int, optional
        The number of decimal places to use for tracking miliseconds.
        The default is 0

    Returns
    -------
    printed : str
        Printed message with time.

```{python, print_message_with_time, echo=TRUE}
from tsdst.utils import print_message_with_time

when_i_started = 0
when_i_stopped = 189

print_message_with_time("This is a message...", ts=when_i_started, te=when_i_stopped)
```

### print_time

Wrapper for print_message_with_time. Shortened for ease of use.

    Parameters
    ----------
    *args : 
        Positional arguments (passed to print_message_with_time).
    **kwargs : 
        Keyword Arguments (passed to print_message_with_time).

    Returns
    -------
    None.

```{python, print_time, echo=TRUE}
from tsdst.utils import print_time

when_i_started = 0
when_i_stopped = 189

print_time("This is a message...", ts=when_i_started, te=when_i_stopped)
```

### reshape_to_vect

Flatten or reshape an array to be a vector (or transpose an already flat array).

This is a convenience function for the bPCA algorithm, but it may have other uses. 

    Parameters
    ----------
    ar : numpy array
        An array to be reshaped.
    axis : int, optional
        The direction to flatten. The default is 1.

    Returns
    -------
    ar : numpy array
        The flattened array.

### sample

Draw a sample (with or without replacement) of size n. Mimics the R sample function

    Parameters
    ----------
    array : list or list-like
        The array to sample from.
    nsamples : int
        The number of samples to return.
    prob : list or list-like, optional
        The probability of each element in array. Must be of the same length
        as array. If None, all elements have equal probability.
        The default is None.
    replace : bool, optional
        Sample with replacement. The default is False.
    random_state : int, optional
        Set seed for reproducible results. The default is None.

    Raises
    ------
    ValueError
        Raises error if array lengths are not equal or if sampling without
        replacement is not possible.

    Returns
    -------
    samp : list
        List of sampled values.

### save_checkpoint

Save a python object to a pickle file as a checkpoint. Log message as logfile with the export.

    Parameters
    ----------
    obj : any python object (must be pickleable)
        The object to pickle.
    msg : str
        The message you want to print.
    ts : float
        Start time in seconds, or elapsed time if te is None.
    te : float, optional
        End time in seconds. The default is None.
    decimals : int, optional
        The number of decimal places to use for tracking miliseconds.
        The default is 0
    display_realtime : bool, optional
        Display the system (calendar) time as part of the output.
        The default is True.
    backsn : bool, optional
        Add '\\n'. The default is False.
    log : bool, optional
        Save message to a log file. The default is False.
    log_dir : str, optional
        The directory for the log file. The default is "log".
    log_filename : str, optional
        The name of the logfile. The default is "pmwt.log".
    log_args : str, optional
        The read/write specification, for example, wb, w, a, etc.
        The default is "a" for append. See open function in python for more
        details.
    time_first : bool, optional
        Place time at the begininng of the message. The default is False.
    checkpoint_dir : str, optional
        The directory to save the checkpoint. The default is "checkpoints".
    checkpoint_filename : str, optional
        The filename for the checkpoint. The default is "chkpnt".
    checkpoint_extension : str, optional
        The file extension for the checkpoint. The default is "pkl".
    checkpoint_args : str, optional
        The read/write specification, for example, wb, w, a, etc.
        See open function in python for more details.
        The default is "wb" for write binary.

    Returns
    -------
    None.

### toframe

Convert numpy array to pandas dataframe using past dataframe structure. 

Useful when you want to store the dataframe values as a seperate numpy array, and make adjustments to the values, but want to convert back to a dataframe once the calculations are completed. 

    Parameters
    ----------
    todf : numpy array
        Data to convert to pandas dataframe.
    fromdf : pandas dataframe
        Dataframe containing structure to convert back to.

    Returns
    -------
    pandas dataframe
        The converted dataframe.

```{python, toframe}
import numpy as np
import pandas as pd
from tsdst.utils import toFrame


X = pd.DataFrame(np.array([[1, 2], [3, 4]]), columns=['col1', 'col2'])

# Convert to numpy array
Y = X.values

# Do some calculation
Y = Y*50

#reconstruct array
Y = toFrame(Y, X)
```

### updateProgBar

Create and update a progress bar. Place this function anywhere in a loop where you want to keep track of the loop's progress.

    Parameters
    ----------
    curIter : int
        The current iteration.
    totalIter : int
        The total number of iterations. 
    t0 : numeric
        The start time of the operation (in seconds).
    barLength : int, optional
        The length of the progress bar. The default is 20.
    decimals : int, optional
        The number of decimal places to use for tracking miliseconds.
        The default is 0

    Returns
    -------
    None.

```{python, updateProgBar}
from timeit import default_timer as dt
from tsdst.utils import updateProgBar

t0 = dt()
for i in range(5):
    updateProgBar(i+1, totalIter=5, t0=t0)
```

# Appendix

## Reliability Basics

Reliability: Probability of components, parts, or systems to perform their required functions for a desired period of time

* In the general sense, Reliability does not account for repairs
    + Repairable systems can be accounted for, but it's not always inherent in the way we typically calculate reliability
* Reliability does account for the time to failure of something while it is operating
* Can be defined as time to failure (TTF)

Maintainability: Probability of performing a successful repair in a given time

* This is the time between failure and replacement/repair
* Can be defined as time to repair (TTR)

Availability: Probability that the system is operating properly when requested for use when requested for

* In other words, the probability that it is not currently failed or in repair when asked to use
* Can be defined as time between failure (TBF)
* Availability is a function of reliability and maintainability
    + $TBF = TTF + TTR$

## Hazard Function

The hazard function is defined as the instantaneous failure rate, which is the propensity for the item to fail in the next shortest instant of time.

$$ \lim_{\Delta t \rightarrow 0}{\frac{F(t + \Delta t) - F(t)}{\Delta t}}\frac{1}{R(t)} = \frac{f(t)}{R(t)}$$

## Mean Residual Life

The expected TTF of a device that has survived to time t

$$M(t) = \frac{1}{R(t)} \int_{t}^{\infty}sf(t + s)ds$$

### Censoring

Let $t_a$ be uncensored, and let $t_b$ be censored. Then:

$$L(\theta | t) = \prod f(t | \theta)$$

Uncensored: $L(\theta | t) = \prod f(t_a | \theta) \prod f(t_b | \theta) = \prod f(t | \theta)$

Left Censored: $L(\theta | t) = \prod f(t_a | \theta) \prod F(t_b | \theta)$

Interval Censored: $L(\theta | t) = \prod f(t_a | \theta) \prod \Big[ F(t_R | \theta) - F(t_L | \theta) \Big]$

Right Censored: $L(\theta | t) = \prod f(t_a | \theta) \prod \Big[1 - F(t_b | \theta) \Big]$

## Derivations

### Some Notes on Over-dispersion

There are two main forms of over-dispersion: multiplicative and additive. While additive over-dispersion is generally more desirable, multiplicative is more computationally feasible, especially in a bayesian or MCMC context.

In additive over-dispersion, the dispersion term is set as part of the linear (or generalized linear) model, such that every $i^{th}$ observation has a dispersion parameter, i.e. $\hat{Y_i} = X_i \beta + \epsilon_i$, where $\epsilon$ is not to be confused with the error (or prediction residuals) from $\hat{\epsilon} = Y - \hat{Y}$. This form of over-dispersion is more desirable because it provides an easy way to directly understand which observations (or perhaps groups) of observations are over-dispersed. For example, if you were modeling Poisson counts of car crashes each day, it would be expected that the crashes occurring near Christmas would be dispersed differently because they occur near a holiday. Therefore, upon inspection, we would expect the dispersion parameter to be higher for these observations. However, for 800 observations, this essentially adds 800 parameters to the model since the vector $\epsilon$ of dispersion parameters will need to be optimized and solved for individually. This already adds a lot of extra parameters in a classical context, but in a Bayesian context, this can become overwhelming, both in calculation and memory (for storing the posterior distributions for each observation/sample combination).

In multiplicative over-dispersion, there is only one extra parameter to solve for, because you are adjusting the model rather than the data. For example, in a Poisson model, the mean and variance are $\mu = \lambda$ and $\sigma^2 = \lambda$, so to account for over-dispersion, and adjustment is made to the variance such that $\sigma^2 = \phi \lambda$, since the variability in the data is different than the variation we would expect from the model ($\phi>1$ for over-dispersion, $\phi<1$ for under-dispersion). 

For simplicity, the code in this package uses a multiplicative form of over-dispersion.

### Some Notes on Linear Regression

In linear algebra, there is a common equation at the center of most problems, which is $Ax = b$. For more common use with statistis, we will rewrite this as $X\beta = y$. If $X$ and $\beta$ are known, the two matrices can be multiplied together to get an exact solution of $y$. If $X$ and $y$ are known (and $y$ is a linear combination of $X\beta$), $\beta$ can be solved using row reduction. Sometimes this results in an exact solution (i.e. zero residual error), however, in practice, the system will either be inconsistent (the usual, since there are usually more observations than features) or have infinite solutions (happens when there's more features than observations).  In the latter scenario, linear regression is usually avoided since this causes some weights to be free variables (any set of values could produce the same answer). If the system is inconsistent (which when we gather data, we typically assume from the beginning), then a "least squares" solution for $\beta$ can be obtained by rearranging the equation and solving for $\beta$

$$X\beta = y$$
$$X^T X \beta = X^T y$$
$$(X^T X)^{-1} X^T X \beta = (X^T X)^{-1} X^T y$$
$$\beta = (X^T X)^{-1} X^T y$$

Note that $(X^T X)^{-1} X^T$ is the pseudo-inverse of $X$. For those unfamiliar with matrix operations, the corrollary in non-matrix terms could be described as follows: ($(X^T X)^{-1} X^{T}$ is like $\frac{X^T}{X^T X}$, which could be thought of as $\frac{X}{X^2}$, which then reduces to $\frac{1}{X}$). If the matrix $X$ was square, a solution of the form $\beta = X^{-1}y$ might be possible (i.e. if $X$ was invertible), but this is rarely the case in practice (hence, the pseudo-inverse). Since in practice we know that we probably won't find a $\beta$ such that $X\beta = y$, we can still find $\beta \approx (X^T X)^{-1} X^T y$ since we know $X$ and $y$. When we solve it this way, it's just matrix multiplication (instead of row reduction), but it's not guaranteeing the theoretical solution we were hoping for in $X\beta = y$, i.e. we still don't know if the system is inconsistent or exactly one solution exists. Knowing that the $\beta$ calculated from $(X^T X)^{-1} X^T y$ is approximate, we can better rewrite the equation as $\hat{\beta} = (X^T X)^{-1} X^T y$. Once we calculate $\hat{\beta}$, we can also calculate some approximate $y$s to match the approximate $\beta$s, as $\hat{y} = X\hat{\beta}$. $\epsilon = y - \hat{y}$ can then be used to represent the error or difference of the estimated solution $X\hat{\beta} = \hat{y}$ and the observed solution $X\hat{\beta} + \epsilon = y$. Note also how we derive the leverage statistic from here:

$$\hat{\beta} = (X^T X)^{-1} X^T y$$
$$X\hat{\beta} = X(X^T X)^{-1} X^T y$$
$$\hat{y} = \hat{H}y$$

For a more detailed explanation of Linear Algebra, see [@linearAlgebra].

### Truncated Distributions
With respect to reliability,

Let A: Experiencing a failure between $t_1$ and $t_n$, where n is the future time of failure

Let B: Lived to time $t_0$

Then:

$$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \cap A)}{P(B)} = \frac{P(B | A)P(A)}{P(B)}$$

Since $P(B|A) = 1$ because it is guarenteed to have survived to $t_0$ if it failed between $t_1$ and $t_n$, then:

$$P(A|B) = \frac{P(A)}{P(B)} = \frac{F(t_n) - F(t_1)}{1 - F(t_0)}$$

Knowing that a part survived to time $t$, you can calculate the probability of it failing in the future.

The above example demonstrates how to derive a truncated distribution. Please note thaty this is not the only possible truncation. For example, you could easily truncate a distribution based on left censored data rather than right censored, or be interested in removing the top of the distribution rather than the bottom, etc.

By way of formal definition, a truncated distribution is defined as follows:

PDF: $\frac{g(x)}{F(b)-F(a)}$

CDF: $\frac{\int^{x}_{a}{g(t)dt}}{F(b)-F(a)} = \frac{F(x) - F(a)}{F(b) - F(a)}$

### MLE derivation

Derivation of the MLE of a right censored exponential distribution:

Let:

* $n$ = number of non-censored data points
* $m$ = number of censored data points
* $\theta$ = parameters of the distribution

Then:

$$L(\theta | x) = \prod_{t = 1}^n f(x_{t} | \theta) \prod_{r=n+1}^{m} (1 - F(x_r | \theta))$$

$$ \ln(L(\theta | x)) = \ln \Big( \prod_{t = 1}^n f(x_{t} | \theta) \prod_{r=n+1}^{m} (1 - F(x_r | \theta)) \Big)$$
$$ \ln(L(\theta | x)) = \ln \Big( \prod_{t = 1}^n f(x_{t} | \theta) \Big) + \ln \Big( \prod_{r=n+1}^{m} (1 - F(x_r | \theta)) \Big)$$

$$ \ln(L(\theta | x)) = \sum_{t = 1}^n \ln \Big( f(x_{t} | \theta) \Big) + \sum_{r=n+1}^{m} \ln \Big( (1 - F(x_r | \theta)) \Big)$$
From this point, you could solve this equation for any given distribution. Passing the negative log-likelihood to [max_like] would solve it numerically. The rest of this example is specific to the exponential distribution.

$$ \ln(L(\theta | x)) = \sum_{t = 1}^n \ln \Big( \lambda e^{-\lambda x_{t}} \Big) + \sum_{r=n+1}^{m} \ln \Big( (1 - (1 - e^{-\lambda x_{r}})) \Big) = \sum_{t = 1}^n \ln \Big( \lambda e^{-\lambda x_{t}} \Big) + \sum_{r=n+1}^{m}  -\lambda x_{r} $$

$$ \ln(L(\theta | x)) = n \ln{\lambda} + \sum_{t = 1}^n \Big( -\lambda x_{t} \Big) + \sum_{r=n+1}^{m}  -\lambda x_{r} $$

$$ \ln(L(\theta | x)) = n \ln(\lambda) - \lambda \Big( \sum_{t = 1}^n   x_{t}  + \sum_{r=n+1}^{m} x_{r} \Big)$$
$$ \frac{d}{d \lambda}\ln(L(\theta | x)) = \frac{n}{\lambda} - \Big( \sum_{t = 1}^n   x_{t}  + \sum_{r=n+1}^{m} x_{r} \Big)$$
Set the derivative to zero

$$ \frac{d}{d \lambda}\ln(L(\theta | x)) = 0 = \frac{n}{\lambda} - \Big( \sum_{t = 1}^n   x_{t}  + \sum_{r=n+1}^{m} x_{r} \Big)$$

$$ \Big( \sum_{t = 1}^n   x_{t}  + \sum_{r=n+1}^{m} x_{r} \Big) = \frac{n}{\lambda}$$

$$ \lambda_{MLE} = \frac{n}{\Big( \sum_{t = 1}^n   x_{t}  + \sum_{r=n+1}^{m} x_{r} \Big)}$$

For likelihood functions with more than one parameter, there is not always an easy derivation, and sometimes no closed form solution. In these cases, it is recommended to use numerical solvers such as Nelder-Mead. 

## NHPP Functions

Power Law Intensity function: $\lambda(t) = \frac{\phi}{\eta} \Big(\frac{t}{\eta} \Big)^{\phi - 1}$

Mean number of failures up to time $t$: $\Lambda (t) = \int_{-\infty}^{t} \lambda(t) dt = \Big( \frac{t}{\eta} \Big) ^{\phi}$

Likelihood (PLP): 

$n_i$ = observed counts in the interval $(a_i, b_i]$, and $i = 1,...,m$ 

$$P(n_i | \Lambda(a_i), \Lambda(b_i)) = \prod_{i=1}^m \frac{[\Lambda(b_i) - \Lambda(a_i)]^{n_i} e^{[-(\Lambda(b_i) - \Lambda(a_i))]}}{n_{i}!}$$

Gamma Prior with a variable change: 

$$P(\eta | \phi) = \frac{\phi \Big(\frac{\mu}{\sigma^{2}} \Big)^{({\frac{\mu}{\sigma}})^2} t^{\phi(\frac{\mu}{\sigma})^{2}} \eta^{-\phi(\frac{\mu}{\sigma})^{2} - 1} e^{[-\frac{\mu}{\sigma^{2}} (\frac{t}{\eta})^\phi]}}{\Gamma[(\frac{\mu}{\sigma})^{2}]}$$

Variable Change:

$$\phi = \frac{b\theta + a}{1 + \theta}, \ \ \ \ \ \eta = \eta$$

$$\frac{\partial \phi}{\partial \theta} \frac{\partial \eta}{\partial \eta} - \frac{\partial \theta}{\partial \eta}\frac{\partial \eta}{\partial \theta} = \frac{b - a}{(1 + \theta)^2}$$

See [@bayesrelia] for more information.

## General Stats Information

### Classification

#### Confusion Matrices

A typical confusion matrix is as follows:

```{r, echo=FALSE}
confMat <- matrix(c("True Positive (TP)", "False Positive (FP, Type I)", "False Negative (FN, Type II)", "True Negative (TN)"), nrow=2, byrow=TRUE)
row.names(confMat) <- c("Observed/Actual Positive (OP)", "Observed/Actual Negative (ON)")
colnames(confMat) <- c("Predicted Positive (PP)", "Predicted Negative (PN)")
kable(confMat)
```

In classification, there are several metrics of interest that can be derived from a confusion matrix. 

* Accuracy
    - This is the most common measure. Simply, it is the number of predictions you got right divided by the the total number of observations
    - $\frac{\Sigma{TP} + \Sigma{TN}}{\Sigma{Obs}}$
* Recall - Sensitivity - True Positive Rate - Power - Probability of Detection
    - The proportion of actual positives that are correctly predicted
    - The probability of detecting the condition when it's actually there
    - For power ($1 - \beta$), the probability of rejecting the null hypothesis when the alternate hypothesis is true. Since type II error is failing to reject a false null hypothesis (or rejecting a true alternate hypothesis), as power increases, the probability of making a type II error decreases. 
* False Negative Rate - Miss Rate
    - proportion of times you "miss" predicting a condition that is actually true
    - probability of making a type II error

## Acknowledgments

I'd like to thank all my friends and colleagues over the years who have helped me put this together, namely:
    
    * Mark Lyman
    * Joel Linford
    * Darren Maynard
    * Kevin Martin

# References


